{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BAGIAN 0: PENGATURAN PROYEK"
      ],
      "metadata": {
        "id": "tbx3-U2a6AvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 0.1: Hubungkan Google Drive & Buat Struktur Folder Proyek"
      ],
      "metadata": {
        "id": "z-6M36fx6FH4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wccmxJzM6ATw"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SEL 0.1: Hubungkan Google Drive & Buat Struktur Folder Proyek\n",
        "# =============================================================================\n",
        "print(\"--- SEL 0.1: Memulai Inisiasi Proyek ---\")\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "def setup_project_structure():\n",
        "    \"\"\"\n",
        "    Menghubungkan Google Colab ke Google Drive dan membuat struktur folder\n",
        "    yang diperlukan untuk proyek prediksi risiko kredit.\n",
        "\n",
        "    Struktur Folder:\n",
        "    - 00_data_raw: Untuk menyimpan dataset asli.\n",
        "    - 01_data_processed: Untuk menyimpan data yang telah dibersihkan dan diproses.\n",
        "    - 02_models: Untuk menyimpan file model yang telah dilatih.\n",
        "    - 03_results: Untuk menyimpan hasil evaluasi, metrik, dan visualisasi.\n",
        "    - 04_notebooks: (Opsional) Untuk menyimpan versi notebook ini.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Hubungkan ke Google Drive\n",
        "        print(\"üìÇ Menghubungkan ke Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"   -> Google Drive berhasil terhubung.\")\n",
        "\n",
        "        # 2. Definisikan Path Proyek Utama\n",
        "        # Ganti 'Credit_Risk_Prediction' jika Anda ingin nama folder yang berbeda\n",
        "        PROJECT_ROOT = \"/content/drive/MyDrive/Colab_Projects/Credit_Risk_Prediction#1\"\n",
        "        print(f\"\\nüìÅ Direktori utama proyek diatur ke: {PROJECT_ROOT}\")\n",
        "\n",
        "        # 3. Definisikan dan Buat Struktur Sub-folder\n",
        "        subdirectories = {\n",
        "            \"DATA_RAW_DIR\": \"00_data_raw\",\n",
        "            \"DATA_PROCESSED_DIR\": \"01_data_processed\",\n",
        "            \"MODELS_DIR\": \"02_models\",\n",
        "            \"RESULTS_DIR\": \"03_results\",\n",
        "            \"NOTEBOOKS_DIR\": \"04_notebooks\"\n",
        "        }\n",
        "\n",
        "        print(\"\\nüõ†Ô∏è  Membuat struktur folder di Google Drive...\")\n",
        "        os.makedirs(PROJECT_ROOT, exist_ok=True)\n",
        "\n",
        "        # Membuat setiap sub-folder dan menyimpannya sebagai variabel global\n",
        "        for dir_variable, dir_path in subdirectories.items():\n",
        "            full_path = os.path.join(PROJECT_ROOT, dir_path)\n",
        "            os.makedirs(full_path, exist_ok=True)\n",
        "            globals()[dir_variable] = full_path\n",
        "            print(f\"   -> Folder '{dir_path}' siap di: {full_path}\")\n",
        "\n",
        "        print(\"\\n‚úÖ Inisiasi Proyek Selesai. Semua struktur folder telah siap.\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå GAGAL saat inisiasi proyek. Error: {e}\")\n",
        "        return False\n",
        "\n",
        "# Jalankan fungsi setup\n",
        "setup_project_structure()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 0.2: Instalasi dan Impor Pustaka"
      ],
      "metadata": {
        "id": "Qv0TNdrH6Iwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 0.2: Instalasi dan Impor Pustaka\n",
        "# =============================================================================\n",
        "print(\"--- SEL 0.2: Memulai Instalasi dan Impor Pustaka ---\")\n",
        "\n",
        "import sys\n",
        "\n",
        "# --- 1. Instalasi Pustaka yang Dibutuhkan ---\n",
        "# Kita menggunakan --quiet untuk menjaga output tetap bersih.\n",
        "# CatBoost, XGBoost, dan LightGBM adalah model boosting yang akan kita gunakan.\n",
        "# SHAP diperlukan untuk interpretasi model (XAI).\n",
        "print(\"üì¶ Menginstal pustaka yang diperlukan (catboost, xgboost, lightgbm, shap)...\")\n",
        "!{sys.executable} -m pip install pandas numpy scikit-learn matplotlib seaborn catboost xgboost lightgbm shap --quiet\n",
        "print(\"   -> Instalasi selesai.\")\n",
        "\n",
        "# --- 2. Impor Pustaka Utama ---\n",
        "print(\"\\nüìö Mengimpor pustaka-pustaka utama...\")\n",
        "\n",
        "# Pustaka Dasar untuk Manipulasi Data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Pustaka untuk Visualisasi\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Pustaka Scikit-learn untuk Preprocessing, Pemodelan, dan Evaluasi\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "# Pustaka untuk Model Machine Learning\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "\n",
        "# Pustaka untuk Interpretasi Model\n",
        "import shap\n",
        "\n",
        "# Pustaka Utilitas\n",
        "import os\n",
        "import joblib # Untuk menyimpan dan memuat model/pipeline\n",
        "import warnings\n",
        "\n",
        "# --- 3. Konfigurasi Lingkungan ---\n",
        "print(\"\\n‚öôÔ∏è  Melakukan konfigurasi lingkungan...\")\n",
        "\n",
        "# Mengatur agar semua kolom Pandas ditampilkan\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Mengatur gaya plot default\n",
        "sns.set_style('whitegrid')\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "\n",
        "# Menonaktifkan peringatan yang tidak krusial untuk menjaga kebersihan output\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "print(\"\\n‚úÖ Semua pustaka telah diimpor dan lingkungan telah dikonfigurasi.\")\n"
      ],
      "metadata": {
        "id": "c0SvSXvk6CZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BAGIAN 1: PEMUATAN DATA & PEMERIKSAAN AWAL"
      ],
      "metadata": {
        "id": "7HEULm0Y6MdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 1.1: Muat Dataset dari Google Drive"
      ],
      "metadata": {
        "id": "k9kvT63z6SBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 1.1: Muat Dataset dari Google Drive\n",
        "# =============================================================================\n",
        "print(\"--- SEL 1.1: Memulai Pemuatan Dataset ---\")\n",
        "\n",
        "# Pastikan variabel path dari SEL 0.1 tersedia\n",
        "if 'DATA_RAW_DIR' not in globals():\n",
        "    print(\"‚ùå ERROR: Variabel path 'DATA_RAW_DIR' tidak ditemukan.\")\n",
        "    print(\"   -> Harap jalankan SEL 0.1 untuk menginisialisasi struktur proyek terlebih dahulu.\")\n",
        "else:\n",
        "    try:\n",
        "        # --- 1. Definisikan Lokasi dan Nama File ---\n",
        "        # Ganti 'loan_data_2007_2014.csv' jika nama file Anda berbeda\n",
        "        file_name = \"loan_data_2007_2014.csv\"\n",
        "        file_path = os.path.join(DATA_RAW_DIR, file_name)\n",
        "        print(f\"üìñ Mencoba memuat file dari: {file_path}\")\n",
        "\n",
        "        # --- 2. Periksa Keberadaan File ---\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"File '{file_name}' tidak ditemukan di direktori '00_data_raw'. \"\n",
        "                                    \"Harap pastikan file sudah diunggah ke Google Drive.\")\n",
        "\n",
        "        # --- 3. Muat Data ke DataFrame ---\n",
        "        df_raw = pd.read_csv(file_path)\n",
        "        print(\"\\n‚úÖ Dataset berhasil dimuat ke dalam DataFrame 'df_raw'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå GAGAL memuat dataset. Error: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "yoDMgjYb6Sfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 1.2: Pemeriksaan Awal Struktur Data"
      ],
      "metadata": {
        "id": "RXmfhHZH6UzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 1.2: Pemeriksaan Awal Struktur Data (Disempurnakan)\n",
        "# =============================================================================\n",
        "print(\"--- SEL 1.2: Melakukan Pemeriksaan Awal Struktur Data ---\")\n",
        "\n",
        "# Pastikan DataFrame 'df_raw' sudah ada\n",
        "if 'df_raw' in globals():\n",
        "    try:\n",
        "        # --- 1. Tampilkan Dimensi DataFrame ---\n",
        "        print(f\"Bentuk (Dimensi) Data: {df_raw.shape[0]} baris dan {df_raw.shape[1]} kolom.\")\n",
        "\n",
        "        # --- 2. Tampilkan 5 Baris Pertama ---\n",
        "        print(\"\\n--- 5 Baris Pertama Data ---\")\n",
        "        display(df_raw.head())\n",
        "\n",
        "        # --- 3. Tampilkan 5 Baris Terakhir ---\n",
        "        print(\"\\n--- 5 Baris Terakhir Data ---\")\n",
        "        display(df_raw.tail())\n",
        "\n",
        "        # --- 4. Fungsi Ringkasan Komprehensif (Saran Tambahan) ---\n",
        "        def summarize_df(df):\n",
        "            \"\"\"\n",
        "            Membuat ringkasan DataFrame yang mencakup tipe data, nilai hilang,\n",
        "            persentase hilang, jumlah nilai unik, dan contoh nilai.\n",
        "            \"\"\"\n",
        "            print(\"\\n--- Ringkasan Komprehensif DataFrame ---\")\n",
        "            summary = pd.DataFrame({\n",
        "                'Dtype': df.dtypes,\n",
        "                'Missing Values': df.isnull().sum(),\n",
        "                'Missing %': (df.isnull().sum() / len(df)) * 100,\n",
        "                'N-Unique': df.nunique(),\n",
        "                'Sample Values': [df[col].dropna().unique()[:3] for col in df.columns]\n",
        "            })\n",
        "            return summary\n",
        "\n",
        "        # Tampilkan ringkasan menggunakan fungsi yang baru dibuat\n",
        "        df_summary = summarize_df(df_raw)\n",
        "        display(df_summary)\n",
        "\n",
        "        # --- 5. Tampilkan Informasi Umum (Tipe Data & Nilai Non-Null) ---\n",
        "        # Ini tetap berguna untuk melihat penggunaan memori\n",
        "        print(\"\\n--- Informasi Umum DataFrame (Tipe Data & Penggunaan Memori) ---\")\n",
        "        df_raw.info(verbose=False) # Menggunakan verbose=False karena detail sudah ada di ringkasan\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå GAGAL melakukan pemeriksaan awal. Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: DataFrame 'df_raw' tidak ditemukan. Harap jalankan SEL 1.1 terlebih dahulu.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "qmGSVeYq6VRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BAGIAN 2: ANALISIS DATA EKSPLORATIF (EDA)"
      ],
      "metadata": {
        "id": "TDPE6vr56a4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 2.1: Analisis Statistik Deskriptif & Identifikasi Nilai Hilang"
      ],
      "metadata": {
        "id": "l_8N6yWF6hN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 2.1: Analisis Statistik Deskriptif & Identifikasi Nilai Hilang\n",
        "# =============================================================================\n",
        "print(\"--- SEL 2.1: Memulai Analisis Statistik Deskriptif & Nilai Hilang ---\")\n",
        "\n",
        "# Pastikan DataFrame 'df_raw' sudah ada\n",
        "if 'df_raw' in globals():\n",
        "    try:\n",
        "        # --- 1. Statistik Deskriptif untuk Kolom Numerik ---\n",
        "        print(\"\\n--- Statistik Deskriptif (Kolom Numerik) ---\")\n",
        "        # Menggunakan .T (transpose) agar lebih mudah dibaca\n",
        "        display(df_raw.describe().T)\n",
        "\n",
        "        # --- 2. Menghitung Persentase Nilai Hilang (Missing Values) ---\n",
        "        print(\"\\n--- Persentase Nilai Hilang per Kolom ---\")\n",
        "        missing_values = df_raw.isnull().sum()\n",
        "        missing_percentage = (missing_values / len(df_raw)) * 100\n",
        "\n",
        "        # Gabungkan jumlah dan persentase untuk tampilan yang lebih baik\n",
        "        missing_info = pd.DataFrame({\n",
        "            'Jumlah Hilang': missing_values,\n",
        "            'Persentase Hilang (%)': missing_percentage\n",
        "        })\n",
        "\n",
        "        # Tampilkan hanya kolom yang memiliki nilai hilang, diurutkan dari yang terbesar\n",
        "        missing_info = missing_info[missing_info['Jumlah Hilang'] > 0].sort_values(by='Persentase Hilang (%)', ascending=False)\n",
        "\n",
        "        if missing_info.empty:\n",
        "            print(\"‚úÖ Tidak ada nilai yang hilang dalam dataset.\")\n",
        "        else:\n",
        "            print(f\"Ditemukan {len(missing_info)} kolom dengan nilai yang hilang.\")\n",
        "            display(missing_info)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå GAGAL melakukan analisis. Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: DataFrame 'df_raw' tidak ditemukan. Harap jalankan SEL 1.1 & 1.2 terlebih dahulu.\")\n"
      ],
      "metadata": {
        "id": "eiw0IbbR6d5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 2.2: Analisis Variabel Target (loan_status)"
      ],
      "metadata": {
        "id": "OgfCtQoT6i9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 2.2: Analisis Variabel Target (loan_status)\n",
        "# =============================================================================\n",
        "print(\"--- SEL 2.2: Memulai Analisis Variabel Target ---\")\n",
        "\n",
        "if 'df_raw' in globals():\n",
        "    print(\"\\n--- Distribusi Nilai pada Kolom 'loan_status' ---\")\n",
        "    status_counts = df_raw['loan_status'].value_counts(normalize=True) * 100\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=status_counts.index, y=status_counts.values, palette='mako')\n",
        "    plt.title('Distribusi Status Pinjaman (Loan Status)')\n",
        "    plt.ylabel('Persentase (%)')\n",
        "    plt.xlabel('Status Pinjaman')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.show()\n",
        "\n",
        "    display(status_counts.to_frame(name='Persentase (%)'))\n",
        "else:\n",
        "    print(\"‚ùå ERROR: DataFrame 'df_raw' tidak ditemukan.\")\n"
      ],
      "metadata": {
        "id": "T7kcM0OO6jWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 2.3 (BARU): Analisis Univariat Komprehensif"
      ],
      "metadata": {
        "id": "50NhrMY86ozv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 2.3 (BARU): Analisis Univariat Komprehensif (dengan Penyimpanan)\n",
        "# =============================================================================\n",
        "print(\"--- SEL 2.3: Memulai Analisis Univariat (Distribusi Setiap Fitur) ---\")\n",
        "\n",
        "if 'df_raw' in globals() and 'RESULTS_DIR' in globals():\n",
        "    # --- Persiapan Folder Penyimpanan ---\n",
        "    VIZ_DIR = os.path.join(RESULTS_DIR, \"visualizations/univariat\")\n",
        "    os.makedirs(VIZ_DIR, exist_ok=True)\n",
        "    print(f\"üñºÔ∏è  Visualisasi akan disimpan di: {VIZ_DIR}\")\n",
        "\n",
        "    # Pisahkan kolom numerik dan kategorikal untuk analisis\n",
        "    numeric_cols = df_raw.select_dtypes(include=np.number).columns.tolist()\n",
        "    categorical_cols = df_raw.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    cols_to_exclude = ['id', 'member_id', 'loan_status']\n",
        "    numeric_cols = [col for col in numeric_cols if col not in cols_to_exclude]\n",
        "    categorical_cols = [col for col in categorical_cols if col not in cols_to_exclude]\n",
        "\n",
        "    print(f\"Ditemukan {len(numeric_cols)} fitur numerik dan {len(categorical_cols)} fitur kategorikal untuk dianalisis.\")\n",
        "\n",
        "    # --- 1. Fungsi untuk Analisis Univariat Numerik (dengan Penyimpanan) ---\n",
        "    def plot_numeric_distribution(df, col_list):\n",
        "        print(f\"\\n--- Menganalisis Distribusi {len(col_list)} Fitur Numerik ---\")\n",
        "        for col in col_list:\n",
        "            fig = plt.figure(figsize=(14, 4))\n",
        "\n",
        "            plt.subplot(1, 2, 1)\n",
        "            sns.histplot(df[col], kde=True, bins=50)\n",
        "            plt.title(f'Histogram Distribusi - {col}')\n",
        "\n",
        "            plt.subplot(1, 2, 2)\n",
        "            sns.boxplot(x=df[col])\n",
        "            plt.title(f'Boxplot - {col}')\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Simpan plot ke file\n",
        "            file_path = os.path.join(VIZ_DIR, f\"univariate_dist_{col}.png\")\n",
        "            plt.savefig(file_path, dpi=150, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            print(f\"   -> Plot untuk '{col}' disimpan ke {file_path}\")\n",
        "\n",
        "    # --- 2. Fungsi untuk Analisis Univariat Kategorikal (dengan Penyimpanan) ---\n",
        "    def plot_categorical_distribution(df, col_list, top_n=20):\n",
        "        print(f\"\\n--- Menganalisis Distribusi {len(col_list)} Fitur Kategorikal ---\")\n",
        "        for col in col_list:\n",
        "            fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "            title_suffix = \"\"\n",
        "            if df[col].nunique() > top_n:\n",
        "                top_categories = df[col].value_counts().nlargest(top_n).index\n",
        "                sns.countplot(y=col, data=df[df[col].isin(top_categories)], order=top_categories, palette='mako')\n",
        "                title_suffix = f\" (Top {top_n})\"\n",
        "            else:\n",
        "                sns.countplot(y=col, data=df, order=df[col].value_counts().index, palette='mako')\n",
        "\n",
        "            plt.title(f'Distribusi Fitur - {col}{title_suffix}')\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Simpan plot ke file\n",
        "            file_path = os.path.join(VIZ_DIR, f\"univariate_count_{col}.png\")\n",
        "            plt.savefig(file_path, dpi=150, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            print(f\"   -> Plot untuk '{col}' disimpan ke {file_path}\")\n",
        "\n",
        "    # Jalankan fungsi analisis pada beberapa kolom kunci\n",
        "    key_numeric_to_plot = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'revol_util', 'total_acc']\n",
        "    key_categorical_to_plot = ['grade', 'sub_grade', 'home_ownership', 'verification_status', 'purpose', 'term']\n",
        "\n",
        "    plot_numeric_distribution(df_raw, [col for col in key_numeric_to_plot if col in df_raw.columns])\n",
        "    plot_categorical_distribution(df_raw, [col for col in key_categorical_to_plot if col in df_raw.columns])\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå ERROR: DataFrame 'df_raw' atau path 'RESULTS_DIR' tidak ditemukan.\")\n"
      ],
      "metadata": {
        "id": "dhcjw1wN6nWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 2.4: Analisis Bivariat (Fitur vs. Target) (dengan Penyimpanan)"
      ],
      "metadata": {
        "id": "Tme6XwQf6rmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 2.4: Analisis Bivariat (Fitur vs. Target) (dengan Penyimpanan)\n",
        "# =============================================================================\n",
        "print(\"--- SEL 2.4: Memulai Analisis Bivariat (Hubungan Fitur dengan Risiko Kredit) ---\")\n",
        "\n",
        "if 'df_raw' in globals() and 'RESULTS_DIR' in globals():\n",
        "    # Pastikan folder penyimpanan ada\n",
        "    VIZ_DIR = os.path.join(RESULTS_DIR, \"visualizations/bivariat\")\n",
        "    os.makedirs(VIZ_DIR, exist_ok=True)\n",
        "\n",
        "    df_eda = df_raw.copy()\n",
        "    bad_loan_statuses = ['Charged Off', 'Default', 'Does not meet the credit policy. Status:Charged Off', 'Late (31-120 days)']\n",
        "    df_eda['credit_risk_temp'] = np.where(df_eda['loan_status'].isin(bad_loan_statuses), 1, 0)\n",
        "\n",
        "    print(f\"Proporsi 'Bad Loan' (Risiko=1) dalam data: {df_eda['credit_risk_temp'].mean() * 100:.2f}%\")\n",
        "\n",
        "    # --- 1. Analisis Fitur Kategorikal vs. Target (dengan Penyimpanan) ---\n",
        "    print(\"\\n--- Menganalisis Hubungan Fitur Kategorikal dengan Risiko Kredit ---\")\n",
        "    for col in ['grade', 'term', 'home_ownership', 'verification_status']:\n",
        "        if col in df_eda.columns:\n",
        "            fig = plt.figure(figsize=(10, 5))\n",
        "            risk_by_cat = df_eda.groupby(col)['credit_risk_temp'].mean().sort_values()\n",
        "            sns.barplot(x=risk_by_cat.index, y=risk_by_cat.values, palette='viridis')\n",
        "            plt.title(f'Tingkat Risiko Rata-rata per {col}')\n",
        "            plt.ylabel('Rata-rata Risiko (Bad Loan Rate)')\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "            file_path = os.path.join(VIZ_DIR, f\"bivariate_risk_vs_{col}.png\")\n",
        "            plt.savefig(file_path, dpi=150, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            print(f\"   -> Plot untuk '{col}' disimpan ke {file_path}\")\n",
        "\n",
        "    # --- 2. Analisis Fitur Numerik vs. Target (dengan Penyimpanan) ---\n",
        "    print(\"\\n--- Menganalisis Hubungan Fitur Numerik dengan Risiko Kredit ---\")\n",
        "    for col in ['int_rate', 'annual_inc', 'dti', 'loan_amnt']:\n",
        "        if col in df_eda.columns:\n",
        "            fig = plt.figure(figsize=(8, 5))\n",
        "            plot_col_name = col\n",
        "            if col == 'annual_inc':\n",
        "                sns.boxplot(x='credit_risk_temp', y=np.log1p(df_eda[col]), data=df_eda, palette='pastel')\n",
        "                plt.ylabel(f'log({col})')\n",
        "                plot_col_name = f\"log_{col}\"\n",
        "            else:\n",
        "                sns.boxplot(x='credit_risk_temp', y=col, data=df_eda, palette='pastel')\n",
        "                plt.ylabel(col)\n",
        "\n",
        "            plt.title(f'Distribusi {col} berdasarkan Risiko Kredit')\n",
        "            plt.xticks([0, 1], ['Good Loan', 'Bad Loan'])\n",
        "\n",
        "            file_path = os.path.join(VIZ_DIR, f\"bivariate_box_{plot_col_name}_vs_risk.png\")\n",
        "            plt.savefig(file_path, dpi=150, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            print(f\"   -> Plot untuk '{col}' disimpan ke {file_path}\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: DataFrame 'df_raw' atau path 'RESULTS_DIR' tidak ditemukan.\")\n"
      ],
      "metadata": {
        "id": "nS5ejMHx6qRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 2.5: Analisis Korelasi Mendalam (dengan Penyimpanan)"
      ],
      "metadata": {
        "id": "75PlduUK-c9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 2.5: Analisis Korelasi Mendalam (dengan Penyimpanan)\n",
        "# =============================================================================\n",
        "print(\"--- SEL 2.5: Memulai Analisis Korelasi Mendalam ---\")\n",
        "\n",
        "if 'df_eda' in globals() and 'RESULTS_DIR' in globals():\n",
        "    # --- Persiapan Folder Penyimpanan ---\n",
        "    VIZ_DIR = os.path.join(RESULTS_DIR, \"visualizations/korelasi\")\n",
        "    DATA_DIR = os.path.join(RESULTS_DIR, \"analysis_data\")\n",
        "    os.makedirs(VIZ_DIR, exist_ok=True)\n",
        "    os.makedirs(DATA_DIR, exist_ok=True)\n",
        "    print(f\"üñºÔ∏è  Visualisasi akan disimpan di: {VIZ_DIR}\")\n",
        "    print(f\"üìä Data analisis akan disimpan di: {DATA_DIR}\")\n",
        "\n",
        "    numeric_df = df_eda.select_dtypes(include=np.number)\n",
        "    correlation_matrix = numeric_df.corr()\n",
        "\n",
        "    # --- 1. Heatmap Korelasi (dengan Penyimpanan) ---\n",
        "    fig = plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False)\n",
        "    plt.title('Heatmap Korelasi Fitur Numerik')\n",
        "\n",
        "    file_path_viz = os.path.join(VIZ_DIR, \"correlation_heatmap.png\")\n",
        "    plt.savefig(file_path_viz, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"   -> Heatmap korelasi disimpan ke {file_path_viz}\")\n",
        "\n",
        "    # --- 2. Korelasi Tertinggi dengan Target (dengan Penyimpanan) ---\n",
        "    print(\"\\n--- Fitur dengan Korelasi Tertinggi terhadap 'credit_risk_temp' ---\")\n",
        "    corr_with_target = correlation_matrix['credit_risk_temp'].sort_values(ascending=False).drop('credit_risk_temp')\n",
        "\n",
        "    # Simpan hasil ke CSV\n",
        "    file_path_data = os.path.join(DATA_DIR, \"correlation_with_target.csv\")\n",
        "    corr_with_target.to_csv(file_path_data, header=['correlation'])\n",
        "    print(f\"   -> Data korelasi dengan target disimpan ke {file_path_data}\")\n",
        "\n",
        "    display(corr_with_target.to_frame().head(10))\n",
        "    display(corr_with_target.to_frame().tail(10))\n",
        "\n",
        "    # --- 3. Identifikasi Multikolinearitas Tinggi (dengan Penyimpanan) ---\n",
        "    print(\"\\n--- Pasangan Fitur dengan Multikolinearitas Tinggi (Korelasi > 0.7) ---\")\n",
        "    abs_corr_matrix = correlation_matrix.abs()\n",
        "    corr_pairs = abs_corr_matrix.unstack()\n",
        "    sorted_pairs = corr_pairs.sort_values(kind=\"quicksort\", ascending=False)\n",
        "    high_corr_pairs = sorted_pairs[(sorted_pairs > 0.7) & (sorted_pairs < 1)]\n",
        "\n",
        "    if high_corr_pairs.empty:\n",
        "        print(\"‚úÖ Tidak ditemukan pasangan fitur dengan korelasi absolut > 0.7.\")\n",
        "    else:\n",
        "        print(\"Ditemukan pasangan fitur yang sangat berkorelasi (potensi redundan):\")\n",
        "        high_corr_df = high_corr_pairs.to_frame(name='Correlation').reset_index()\n",
        "        high_corr_df.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
        "\n",
        "        # Simpan hasil ke CSV\n",
        "        file_path_data = os.path.join(DATA_DIR, \"high_multicollinearity_pairs.csv\")\n",
        "        high_corr_df.to_csv(file_path_data, index=False)\n",
        "        print(f\"   -> Data pasangan multikolinearitas tinggi disimpan ke {file_path_data}\")\n",
        "\n",
        "        display(high_corr_df)\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå ERROR: DataFrame 'df_eda' atau path 'RESULTS_DIR' tidak ditemukan.\")\n"
      ],
      "metadata": {
        "id": "Rt7VYWKI-byh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 2.6: Ringkasan EDA & Rencana Aksi (Dinamis dengan Python)"
      ],
      "metadata": {
        "id": "-mq91B-s-q1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 2.6: Ringkasan EDA & Rencana Aksi (Dinamis dengan Python) - (FIXED)\n",
        "# =============================================================================\n",
        "print(\"--- SEL 2.6: Membuat Ringkasan EDA dan Rencana Aksi ---\")\n",
        "\n",
        "class EdaSummary:\n",
        "    def __init__(self):\n",
        "        self.findings = {\n",
        "            \"Target Variable\": [],\n",
        "            \"Columns to Drop\": [],\n",
        "            \"Feature Engineering Ideas\": [],\n",
        "            \"Preprocessing Strategy\": []\n",
        "        }\n",
        "        self.log = []\n",
        "\n",
        "    def add_finding(self, category, description):\n",
        "        if category in self.findings:\n",
        "            self.findings[category].append(description)\n",
        "        else:\n",
        "            print(f\"Warning: Kategori '{category}' tidak ditemukan.\")\n",
        "\n",
        "    def generate_report(self):\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"      RINGKASAN EKSEKUTIF & RENCANA AKSI (Berdasarkan EDA)\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "        for category, items in self.findings.items():\n",
        "            print(f\"### {category.upper()} ###\")\n",
        "            if not items:\n",
        "                print(\"   (Belum ada temuan)\")\n",
        "            for i, item in enumerate(items, 1):\n",
        "                print(f\"   {i}. {item}\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "        print(\"\\n### LOG TEMUAN OTOMATIS ###\")\n",
        "        if not self.log:\n",
        "            print(\"   (Tidak ada log otomatis yang dihasilkan)\")\n",
        "        for entry in self.log:\n",
        "            print(f\"   - {entry}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "# --- Proses Pengisian Ringkasan Secara Dinamis ---\n",
        "# Pastikan variabel dari sel sebelumnya ada\n",
        "if 'df_raw' in globals() and 'missing_info' in globals() and 'high_corr_pairs' in globals():\n",
        "    summary = EdaSummary()\n",
        "\n",
        "    # 1. Analisis Variabel Target\n",
        "    summary.add_finding(\"Target Variable\", \"Definisi 'Bad Loan' (risk=1) adalah status: 'Charged Off', 'Default', 'Late (31-120 days)', dll.\")\n",
        "    summary.add_finding(\"Target Variable\", \"Definisi 'Good Loan' (risk=0) adalah 'Fully Paid'.\")\n",
        "    summary.add_finding(\"Target Variable\", \"Status pinjaman yang masih berjalan ('Current', 'In Grace Period') akan difilter dan tidak digunakan dalam pemodelan.\")\n",
        "\n",
        "    # 2. Analisis Kolom yang Akan Dihapus (Drop)\n",
        "    # Berdasarkan nilai hilang\n",
        "    high_missing_cols = missing_info[missing_info['Persentase Hilang (%)'] > 40].index.tolist()\n",
        "    if high_missing_cols:\n",
        "        summary.add_finding(\"Columns to Drop\", f\"Kolom dengan >40% nilai hilang: {', '.join(high_missing_cols)}.\")\n",
        "        summary.log.append(f\"LOG: Terdeteksi {len(high_missing_cols)} kolom dengan nilai hilang > 40%.\")\n",
        "\n",
        "    # Berdasarkan relevansi/kebocoran data (manual)\n",
        "    leakage_cols = ['funded_amnt', 'funded_amnt_inv', 'total_pymnt', 'total_rec_prncp', 'total_rec_int', 'last_pymnt_d', 'last_pymnt_amnt']\n",
        "    summary.add_finding(\"Columns to Drop\", f\"Kolom yang mengandung kebocoran data (informasi masa depan): {', '.join(leakage_cols)}.\")\n",
        "\n",
        "    # Berdasarkan multikolinearitas\n",
        "    if not high_corr_pairs.empty:\n",
        "        # Logika sederhana: simpan yang pertama, tandai yang kedua untuk dihapus\n",
        "        cols_to_drop_corr = list(set([idx[1] for idx in high_corr_pairs.index]))\n",
        "        summary.add_finding(\"Columns to Drop\", f\"Kolom redundan karena multikolinearitas tinggi (>0.7): {', '.join(cols_to_drop_corr)} (contoh: 'loan_amnt' vs 'funded_amnt').\")\n",
        "        summary.log.append(f\"LOG: Terdeteksi {len(cols_to_drop_corr)} kolom redundan karena korelasi tinggi.\")\n",
        "\n",
        "    # Kolom dengan varians rendah atau tidak relevan\n",
        "    summary.add_finding(\"Columns to Drop\", \"Kolom ID ('id', 'member_id') dan teks bebas ('desc', 'title') akan dihapus.\")\n",
        "\n",
        "    # 3. Rencana Feature Engineering\n",
        "    summary.add_finding(\"Feature Engineering Ideas\", \"Buat 'credit_history_length' dari 'issue_d' dan 'earliest_cr_line'.\")\n",
        "    # INI BARIS YANG DIPERBAIKI: addfinding -> add_finding\n",
        "    summary.add_finding(\"Feature Engineering Ideas\", \"Buat rasio-rasio penting: 'loan_to_income', 'installment_to_income'.\")\n",
        "    summary.add_finding(\"Feature Engineering Ideas\", \"Terapkan transformasi log pada fitur numerik yang miring (skewed) seperti 'annual_inc' dan 'revol_bal'.\")\n",
        "\n",
        "    # 4. Rencana Preprocessing\n",
        "    summary.add_finding(\"Preprocessing Strategy\", \"Imputasi nilai hilang pada fitur numerik menggunakan median (lebih robust terhadap outlier).\")\n",
        "    summary.add_finding(\"Preprocessing Strategy\", \"Imputasi nilai hilang pada fitur kategorikal menggunakan modus ('missing' atau nilai paling umum).\")\n",
        "    summary.add_finding(\"Preprocessing Strategy\", \"Terapkan StandardScaler pada semua fitur numerik setelah imputasi.\")\n",
        "    summary.add_finding(\"Preprocessing Strategy\", \"Terapkan OneHotEncoder pada fitur kategorikal setelah imputasi.\")\n",
        "    summary.add_finding(\"Preprocessing Strategy\", \"Gunakan ColumnTransformer untuk memastikan semua langkah preprocessing digabungkan dalam satu pipeline yang konsisten.\")\n",
        "\n",
        "    # Tampilkan laporan yang dihasilkan\n",
        "    summary.generate_report()\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå ERROR: Pastikan semua sel EDA sebelumnya (2.1 - 2.5) telah dijalankan untuk menghasilkan variabel yang dibutuhkan (df_raw, missing_info, high_corr_pairs).\")\n",
        "\n"
      ],
      "metadata": {
        "id": "A11dlo8I-pY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BAGIAN 3A: PRE-PROCESSING & FEATURE ENGINEERING AWAL (SEBELUM SPLIT)"
      ],
      "metadata": {
        "id": "Y1gwE8RCCu4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 3A.1 (REVISI): Basic Feature Engineering & Row Filtering"
      ],
      "metadata": {
        "id": "E7AVqH4yCz0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 3A.1 (REVISI): Basic Feature Engineering & Row Filtering\n",
        "# =============================================================================\n",
        "print(\"--- SEL 3A.1: Memulai Basic Feature Engineering & Row Filtering ---\")\n",
        "\n",
        "if 'df_raw' in globals():\n",
        "    try:\n",
        "        # --- 1. Buat DataFrame Kerja Utama ---\n",
        "        df_cleaned = df_raw.copy()\n",
        "        print(\"‚úÖ Salinan DataFrame 'df_raw' telah dibuat sebagai 'df_cleaned'.\")\n",
        "\n",
        "        # --- 2. Basic Feature Engineering ---\n",
        "        print(\"\\nüõ†Ô∏è  Memulai pembuatan fitur dasar...\")\n",
        "\n",
        "        # a. Fitur Berbasis Tanggal (dengan logika yang sudah benar)\n",
        "        df_cleaned['issue_d_dt'] = pd.to_datetime(df_cleaned['issue_d'], format='%b-%y', errors='coerce')\n",
        "\n",
        "        def smart_date_converter(row):\n",
        "            issue_date = row['issue_d_dt']\n",
        "            cr_line_str = row['earliest_cr_line']\n",
        "            if pd.isna(issue_date) or pd.isna(cr_line_str): return pd.NaT\n",
        "            cr_line_date = pd.to_datetime(cr_line_str, format='%b-%y', errors='coerce')\n",
        "            if cr_line_date > issue_date: return cr_line_date - pd.DateOffset(years=100)\n",
        "            return cr_line_date\n",
        "\n",
        "        df_cleaned['earliest_cr_line_dt'] = df_cleaned.apply(smart_date_converter, axis=1)\n",
        "        df_cleaned['credit_history_length_years'] = ((df_cleaned['issue_d_dt'] - df_cleaned['earliest_cr_line_dt']).dt.days / 365.25)\n",
        "        print(\"   -> Fitur 'credit_history_length_years' dibuat.\")\n",
        "\n",
        "        # b. Ekstraksi Numerik dari 'term'\n",
        "        df_cleaned['term_in_months'] = df_cleaned['term'].str.extract(r'(\\d+)').astype(float)\n",
        "        print(\"   -> Fitur 'term_in_months' dibuat.\")\n",
        "\n",
        "        # --- 3. Pembuatan Target & Pemfilteran Baris ---\n",
        "        print(\"\\nüéØ Membuat variabel target dan memfilter baris...\")\n",
        "        bad_loan_statuses = ['Charged Off', 'Default', 'Does not meet the credit policy. Status:Charged Off', 'Late (31-120 days)']\n",
        "        df_cleaned['credit_risk'] = np.where(df_cleaned['loan_status'].isin(bad_loan_statuses), 1, 0)\n",
        "\n",
        "        final_statuses = ['Fully Paid'] + bad_loan_statuses\n",
        "        original_rows = len(df_cleaned)\n",
        "        df_cleaned = df_cleaned[df_cleaned['loan_status'].isin(final_statuses)]\n",
        "        print(f\"   -> Baris difilter untuk status final. Jumlah baris: {len(df_cleaned)} (dari {original_rows}).\")\n",
        "\n",
        "        print(\"\\n‚úÖ SEL 3A.1 Selesai. 'df_cleaned' siap untuk langkah selanjutnya.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå GAGAL pada SEL 3A.1. Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: DataFrame 'df_raw' tidak ditemukan.\")\n"
      ],
      "metadata": {
        "id": "4OXzHpXTCxtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 3A.2 (REVISI): Advanced Feature Engineering"
      ],
      "metadata": {
        "id": "XZ4rqJInybZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 3A.2 (REVISI FINAL & TERLENGKAP): Comprehensive Feature Engineering\n",
        "# =============================================================================\n",
        "print(\"--- SEL 3A.2: Memulai Comprehensive Feature Engineering ---\")\n",
        "\n",
        "if 'df_cleaned' in globals():\n",
        "    try:\n",
        "        # --- 1. Ordinal Encoding untuk Grade (diperlukan untuk fitur interaksi) ---\n",
        "        print(\"   -> Menerapkan Ordinal Encoding untuk 'grade' dan 'sub_grade'...\")\n",
        "        grade_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6}\n",
        "        df_cleaned['grade_numeric'] = df_cleaned['grade'].map(grade_map)\n",
        "\n",
        "        sub_grade_sorted = sorted(df_cleaned['sub_grade'].dropna().unique())\n",
        "        sub_grade_map = {sub_grade: i for i, sub_grade in enumerate(sub_grade_sorted)}\n",
        "        df_cleaned['sub_grade_numeric'] = df_cleaned['sub_grade'].map(sub_grade_map)\n",
        "\n",
        "        # --- 2. Membuat Semua Fitur yang Direncanakan ---\n",
        "        print(\"   -> Membuat fitur rasio utang dan pendapatan...\")\n",
        "        df_cleaned['loan_to_income_ratio'] = (df_cleaned['loan_amnt'] / df_cleaned['annual_inc']).replace([np.inf, -np.inf], np.nan)\n",
        "        df_cleaned['installment_to_income_ratio'] = (df_cleaned['installment'] / (df_cleaned['annual_inc'] / 12)).replace([np.inf, -np.inf], np.nan)\n",
        "        df_cleaned['total_balance_to_income_ratio'] = (df_cleaned['tot_cur_bal'] / df_cleaned['annual_inc']).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        print(\"   -> Membuat fitur perilaku kredit...\")\n",
        "\n",
        "        # --- LOGIKA CERDAS UNTUK DELINQUENCY RATE ---\n",
        "        # Hitung rasio seperti biasa\n",
        "        rate = df_cleaned['delinq_2yrs'] / df_cleaned['total_acc']\n",
        "        # Ganti inf (hasil dari pembagian dengan 0) dengan 0, karena jika total_acc=0, delinq_rate juga 0.\n",
        "        # Biarkan NaN tetap NaN untuk diimputasi nanti.\n",
        "        df_cleaned['delinquency_rate'] = rate.replace([np.inf, -np.inf], 0)\n",
        "        # -------------------------------------------\n",
        "\n",
        "        df_cleaned['revolving_balance_to_limit_ratio'] = (df_cleaned['revol_bal'] / df_cleaned['total_rev_hi_lim']).replace([np.inf, -np.inf], np.nan)\n",
        "        df_cleaned['accounts_per_year_of_credit'] = (df_cleaned['total_acc'] / df_cleaned['credit_history_length_years']).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        print(\"   -> Membuat fitur interaksi...\")\n",
        "        df_cleaned['term_x_int_rate'] = df_cleaned['term_in_months'] * df_cleaned['int_rate']\n",
        "        df_cleaned['grade_x_annual_inc'] = df_cleaned['grade_numeric'] * df_cleaned['annual_inc']\n",
        "\n",
        "        print(\"\\n‚úÖ Comprehensive Feature Engineering selesai.\")\n",
        "        print(f\"   -> Dimensi DataFrame sekarang: {df_cleaned.shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå GAGAL pada SEL 3A.2. Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: DataFrame 'df_cleaned' tidak ditemukan. Jalankan SEL 3A.1 terlebih dahulu.\")\n"
      ],
      "metadata": {
        "id": "_Ubpu9bsyaPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 3A.3 (REVISI): Final Column Cleaning"
      ],
      "metadata": {
        "id": "yC4crvh10dbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 3A.3 (REVISI): Final Column Cleaning\n",
        "# =============================================================================\n",
        "print(\"--- SEL 3A.3: Memulai Pembersihan Kolom Final ---\")\n",
        "\n",
        "if 'df_cleaned' in globals():\n",
        "    try:\n",
        "        # --- 1. Definisikan Semua Kolom yang Akan Dihapus ---\n",
        "        # Kolom dengan >40% nilai hilang (berdasarkan EDA awal)\n",
        "        cols_to_drop_missing = ['inq_fi', 'open_rv_24m', 'max_bal_bc', 'all_util', 'inq_last_12m', 'annual_inc_joint', 'verification_status_joint', 'dti_joint', 'total_cu_tl', 'il_util', 'mths_since_rcnt_il', 'total_bal_il', 'open_il_24m', 'open_il_12m', 'open_il_6m', 'open_acc_6m', 'open_rv_12m', 'mths_since_last_record', 'mths_since_last_major_derog', 'desc', 'mths_since_last_delinq', 'next_pymnt_d']\n",
        "\n",
        "        # Kolom yang mengandung kebocoran data\n",
        "        cols_to_drop_leakage = ['funded_amnt', 'funded_amnt_inv', 'total_pymnt', 'total_rec_prncp', 'total_rec_int', 'last_pymnt_d', 'last_pymnt_amnt', 'recoveries', 'collection_recovery_fee', 'out_prncp', 'out_prncp_inv', 'total_pymnt_inv']\n",
        "\n",
        "        # Kolom ID, teks bebas, dan varians rendah\n",
        "        cols_to_drop_other = ['id', 'member_id', 'url', 'title', 'zip_code', 'emp_title', 'addr_state', 'policy_code', 'pymnt_plan', 'Unnamed: 0']\n",
        "\n",
        "        # Kolom yang sudah digunakan untuk membuat fitur baru\n",
        "        cols_to_drop_used_for_fe = ['issue_d', 'earliest_cr_line', 'issue_d_dt', 'earliest_cr_line_dt', 'loan_status', 'term', 'grade', 'sub_grade', 'emp_length']\n",
        "\n",
        "        # Gabungkan semua daftar dan hapus duplikat\n",
        "        total_cols_to_drop = list(set(cols_to_drop_missing + cols_to_drop_leakage + cols_to_drop_other + cols_to_drop_used_for_fe))\n",
        "\n",
        "        # Filter hanya kolom yang benar-benar ada di DataFrame\n",
        "        total_cols_to_drop = [col for col in total_cols_to_drop if col in df_cleaned.columns]\n",
        "\n",
        "        # --- 2. Hapus Kolom ---\n",
        "        df_cleaned.drop(columns=total_cols_to_drop, inplace=True)\n",
        "\n",
        "        print(f\"‚úÖ {len(total_cols_to_drop)} kolom yang tidak diperlukan telah dihapus.\")\n",
        "        print(f\"   -> Dimensi DataFrame sekarang: {df_cleaned.shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå GAGAL pada SEL 3A.3. Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: DataFrame 'df_cleaned' tidak ditemukan. Jalankan SEL 3A.2 terlebih dahulu.\")\n"
      ],
      "metadata": {
        "id": "M5YzZfssLwwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 3A.3.1: Investigasi Dampak Penghapusan Outlier\n",
        "# =============================================================================\n",
        "print(\"--- SEL 3A.4: Memulai Investigasi Dampak Penghapusan Outlier ---\")\n",
        "\n",
        "# Kita akan melakukan ini pada df_cleaned sebelum di-split\n",
        "# untuk mendapatkan gambaran umum.\n",
        "if 'df_cleaned' in globals():\n",
        "    try:\n",
        "        # --- 1. Pilih Fitur Numerik Kunci untuk Diperiksa ---\n",
        "        features_to_check = [\n",
        "            'annual_inc', 'dti', 'revol_bal', 'open_acc', 'total_acc',\n",
        "            'credit_history_length_years', 'loan_amnt', 'int_rate'\n",
        "        ]\n",
        "        # Pastikan semua fitur ada di dataframe\n",
        "        features_to_check = [f for f in features_to_check if f in df_cleaned.columns]\n",
        "        print(f\"Fitur yang akan diperiksa untuk outlier: {features_to_check}\")\n",
        "\n",
        "        # --- 2. Hitung Batas IQR untuk Setiap Fitur ---\n",
        "        outlier_indices = set()\n",
        "\n",
        "        for col in features_to_check:\n",
        "            Q1 = df_cleaned[col].quantile(0.25)\n",
        "            Q3 = df_cleaned[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 3 * IQR\n",
        "            upper_bound = Q3 + 3 * IQR\n",
        "\n",
        "            # Temukan indeks dari baris yang mengandung outlier untuk kolom ini\n",
        "            col_outliers = df_cleaned[\n",
        "                (df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)\n",
        "            ].index\n",
        "\n",
        "            print(f\"   -> Kolom '{col}': Ditemukan {len(col_outliers)} outlier (Batas: [{lower_bound:.2f}, {upper_bound:.2f}])\")\n",
        "\n",
        "            # Tambahkan indeks outlier ke set utama\n",
        "            outlier_indices.update(col_outliers)\n",
        "\n",
        "        # --- 3. Hitung Dampak Total ---\n",
        "        total_outlier_rows = len(outlier_indices)\n",
        "        total_rows = len(df_cleaned)\n",
        "        percentage_impact = (total_outlier_rows / total_rows) * 100\n",
        "\n",
        "        print(\"\\n--- Ringkasan Dampak Penghapusan Outlier ---\")\n",
        "        print(f\"Total baris dalam dataset: {total_rows}\")\n",
        "        print(f\"Jumlah baris unik yang mengandung setidaknya satu outlier: {total_outlier_rows}\")\n",
        "        print(f\"Persentase data yang akan dihapus: {percentage_impact:.2f}%\")\n",
        "\n",
        "        # --- 4. Berikan Rekomendasi Berbasis Data ---\n",
        "        print(\"\\n--- Rekomendasi ---\")\n",
        "        if percentage_impact < 5:\n",
        "            print(\"Dampak penghapusan < 5%. Menghapus baris adalah opsi yang sangat layak dan bersih.\")\n",
        "            print(\"Ini akan menjaga keaslian distribusi data tanpa kehilangan terlalu banyak informasi.\")\n",
        "        elif percentage_impact < 15:\n",
        "            print(\"Dampak penghapusan cukup signifikan (5-15%). Menghapus baris akan menyebabkan kehilangan informasi yang cukup besar.\")\n",
        "            print(\"Disarankan untuk mempertimbangkan Winsorizing atau metode penanganan outlier lainnya untuk mempertahankan ukuran data.\")\n",
        "        else:\n",
        "            print(\"Dampak penghapusan SANGAT TINGGI (> 15%). Menghapus baris sangat tidak disarankan.\")\n",
        "            print(\"Winsorizing atau transformasi (misal, log transform) adalah pendekatan yang jauh lebih baik.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå GAGAL pada SEL 3A.4. Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: DataFrame 'df_cleaned' tidak ditemukan. Harap jalankan SEL 3A.3 terlebih dahulu.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "6C0p77y-cK82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 3A.4 (REVISI): Penghapusan Outlier Ekstrem (Berdasarkan 3 * IQR)"
      ],
      "metadata": {
        "id": "wObbunbfST7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 3A.4 (REVISI YANG BENAR): Penghapusan Outlier Ekstrem Terkontrol\n",
        "# =============================================================================\n",
        "print(\"--- SEL 3A.4: Memulai Penghapusan Baris dengan Outlier Ekstrem (3.0 * IQR) ---\")\n",
        "\n",
        "if 'df_cleaned' in globals():\n",
        "    try:\n",
        "        # --- 1. Pilih HANYA Fitur Inti untuk Diperiksa ---\n",
        "        features_to_check = [\n",
        "            'annual_inc', 'dti', 'revol_bal', 'open_acc', 'total_acc',\n",
        "            'credit_history_length_years', 'loan_amnt', 'int_rate'\n",
        "        ]\n",
        "        features_to_check = [f for f in features_to_check if f in df_cleaned.columns]\n",
        "        print(f\"üéØ Fitur inti yang akan diperiksa untuk outlier: {features_to_check}\")\n",
        "\n",
        "        # --- 2. Dapatkan Indeks Outlier (menggunakan logika 3 * IQR) ---\n",
        "        outlier_indices_to_drop = set()\n",
        "        for col in features_to_check:\n",
        "            Q1 = df_cleaned[col].quantile(0.25)\n",
        "            Q3 = df_cleaned[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 3 * IQR\n",
        "            upper_bound = Q3 + 3 * IQR\n",
        "\n",
        "            col_outliers = df_cleaned[\n",
        "                (df_cleaned[col].notna()) &\n",
        "                ((df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound))\n",
        "            ].index\n",
        "            outlier_indices_to_drop.update(col_outliers)\n",
        "\n",
        "        # --- 3. Hapus Baris Outlier ---\n",
        "        original_rows = len(df_cleaned)\n",
        "        num_outliers = len(outlier_indices_to_drop)\n",
        "\n",
        "        if num_outliers > 0:\n",
        "            print(f\"\\nüóëÔ∏è  Menghapus {num_outliers} baris yang mengandung outlier ekstrem...\")\n",
        "            df_cleaned.drop(index=list(outlier_indices_to_drop), inplace=True)\n",
        "\n",
        "            rows_dropped = original_rows - len(df_cleaned)\n",
        "            print(f\"   -> Jumlah baris sebelum: {original_rows}\")\n",
        "            print(f\"   -> Jumlah baris setelah: {len(df_cleaned)}\")\n",
        "            print(f\"   -> Total baris yang dihapus: {rows_dropped} ({ (rows_dropped/original_rows)*100 :.2f}%)\")\n",
        "        else:\n",
        "            print(\"\\n‚úÖ Tidak ditemukan outlier ekstrem untuk dihapus.\")\n",
        "\n",
        "        print(\"\\n‚úÖ SEL 3A.4 Selesai. Outlier ekstrem telah dihapus.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå GAGAL pada SEL 3A.4. Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: DataFrame 'df_cleaned' tidak ditemukan. Jalankan SEL 3A.3 terlebih dahulu.\")\n"
      ],
      "metadata": {
        "id": "PtVHaFIISSii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 3A.5 (REVISI): Final Checkpoint & Verification\n",
        "# =============================================================================\n",
        "print(\"--- SEL 3A.5: Verifikasi Akhir dan Penyimpanan Checkpoint ---\")\n",
        "\n",
        "if 'df_cleaned' in globals():\n",
        "    # --- 1. Verifikasi Akhir dan Daftar Fitur Final ---\n",
        "    print(f\"\\nüìä Verifikasi akhir DataFrame 'df_cleaned':\")\n",
        "\n",
        "    target_col = 'credit_risk'\n",
        "    features = [col for col in df_cleaned.columns if col != target_col]\n",
        "\n",
        "    print(f\"   -> Dimensi Final: {df_cleaned.shape[0]} baris dan {df_cleaned.shape[1]} kolom.\")\n",
        "    print(f\"   -> Jumlah Fitur Final: {len(features)}\")\n",
        "\n",
        "    numeric_features = df_cleaned[features].select_dtypes(include=np.number).columns.tolist()\n",
        "    categorical_features = df_cleaned[features].select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "    print(f\"\\n   Fitur Numerik Final ({len(numeric_features)}):\")\n",
        "    print(numeric_features)\n",
        "\n",
        "    print(f\"\\n   Fitur Kategorikal Final ({len(categorical_features)}):\")\n",
        "    print(categorical_features)\n",
        "\n",
        "    # --- 2. Simpan Checkpoint Final ---\n",
        "    if 'DATA_PROCESSED_DIR' in globals():\n",
        "        final_checkpoint_path = os.path.join(DATA_PROCESSED_DIR, \"loan_data_model_ready.csv\")\n",
        "        df_cleaned.to_csv(final_checkpoint_path, index=False)\n",
        "        print(f\"\\n\\nüíæ Checkpoint final 'loan_data_model_ready.csv' berhasil disimpan di: {final_checkpoint_path}\")\n",
        "        print(\"   -> DataFrame ini sekarang siap untuk Bagian 3B (Train-Test Split & Pipeline).\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: DataFrame 'df_cleaned' tidak ditemukan. Jalankan sel-sel sebelumnya.\")\n"
      ],
      "metadata": {
        "id": "SCCIqceJ6Cxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 3A.4.1: Inspeksi Hasil Feature Engineering & Pembersihan"
      ],
      "metadata": {
        "id": "_IskLI_90n8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 3A.4.1: Inspeksi Hasil Feature Engineering & Pembersihan\n",
        "# =============================================================================\n",
        "print(\"--- SEL 3A.1.1: Memulai Inspeksi Hasil ---\")\n",
        "\n",
        "if 'df_cleaned' in globals():\n",
        "    try:\n",
        "        # --- 1. Tampilkan 5 Baris Pertama dari Data yang Sudah Diproses ---\n",
        "        print(\"\\n--- Tampilan Awal DataFrame 'df_cleaned' ---\")\n",
        "        display(df_cleaned.head())\n",
        "\n",
        "        # --- 2. Fokus pada Fitur-fitur Baru yang Dibuat ---\n",
        "        new_features = [\n",
        "            'credit_history_length_years',\n",
        "            'loan_to_income_ratio',\n",
        "            'installment_to_income_ratio',\n",
        "            'revolving_util_ratio',\n",
        "            'term_in_months',\n",
        "            'credit_risk' # Juga target kita\n",
        "        ]\n",
        "        # Filter agar hanya menampilkan kolom yang ada\n",
        "        new_features_in_df = [col for col in new_features if col in df_cleaned.columns]\n",
        "\n",
        "        print(\"\\n--- Inspeksi Statistik Deskriptif pada Fitur-fitur Baru ---\")\n",
        "        # .describe() akan memberikan gambaran tentang min, max, mean, dan juga jumlah non-null (count)\n",
        "        display(df_cleaned[new_features_in_df].describe())\n",
        "\n",
        "        # --- 3. Hitung dan Tampilkan Nilai NaN di Setiap Kolom ---\n",
        "        print(\"\\n--- Menghitung Jumlah dan Persentase Nilai NaN pada Data yang Diproses ---\")\n",
        "\n",
        "        nan_counts = df_cleaned.isnull().sum()\n",
        "        nan_percentage = (nan_counts / len(df_cleaned)) * 100\n",
        "\n",
        "        nan_info = pd.DataFrame({\n",
        "            'Jumlah NaN': nan_counts,\n",
        "            'Persentase NaN (%)': nan_percentage\n",
        "        })\n",
        "\n",
        "        # Tampilkan hanya kolom yang masih memiliki nilai NaN, diurutkan\n",
        "        nan_info_filtered = nan_info[nan_info['Jumlah NaN'] > 0].sort_values(by='Jumlah NaN', ascending=False)\n",
        "\n",
        "        if nan_info_filtered.empty:\n",
        "            print(\"‚úÖ Selamat! Tidak ada nilai NaN yang tersisa di dalam DataFrame.\")\n",
        "        else:\n",
        "            print(f\"Ditemukan {len(nan_info_filtered)} kolom yang masih mengandung nilai NaN:\")\n",
        "            display(nan_info_filtered)\n",
        "            print(\"\\nPenjelasan: Nilai-nilai NaN ini adalah normal dan akan ditangani pada tahap preprocessing selanjutnya (Bagian 3B) menggunakan imputer untuk mencegah data leakage.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå GAGAL pada SEL 3A.1.1. Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: DataFrame 'df_cleaned' tidak ditemukan. Harap jalankan SEL 3A.1 terlebih dahulu.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "tJLKiGsYEDJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BAGIAN 3B: FINAL PREPROCESSING & TRAIN-TEST SPLIT"
      ],
      "metadata": {
        "id": "h0itIELcbPmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 3B.1: Pemuatan Data, Pemisahan Fitur-Target, dan Train-Test Split"
      ],
      "metadata": {
        "id": "3ynw_vo_bT7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 3B.1 (REVISI): Pemuatan Data dan Train-Test Split\n",
        "# =============================================================================\n",
        "print(\"--- SEL 3B.1: Memulai Pemuatan Data dan Train-Test Split ---\")\n",
        "\n",
        "# Pastikan Anda sudah menjalankan Bagian 3A yang direvisi (SEL 3A.1, 3A.2, 3A.3)\n",
        "# untuk menghasilkan file CSV yang benar.\n",
        "try:\n",
        "    file_path = os.path.join(DATA_PROCESSED_DIR, \"loan_data_model_ready.csv\")\n",
        "    df_model_ready = pd.read_csv(file_path)\n",
        "    print(f\"‚úÖ DataFrame 'loan_data_model_ready.csv' berhasil dimuat. Dimensi: {df_model_ready.shape}\")\n",
        "\n",
        "    X = df_model_ready.drop(columns=['credit_risk'])\n",
        "    y = df_model_ready['credit_risk']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.1, random_state=42, stratify=y\n",
        "    )\n",
        "    print(\"\\n‚úÖ Data berhasil di-split menjadi set latih dan uji.\")\n",
        "    print(f\"   -> X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "    print(f\"   -> X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå ERROR: File 'loan_data_model_ready.csv' tidak ditemukan. Harap jalankan Bagian 3A terlebih dahulu.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå GAGAL pada SEL 3B.1. Error: {e}\")\n"
      ],
      "metadata": {
        "id": "vTOUVWhDbUPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 3B.2: Pembangunan Pipeline Preprocessing dengan ColumnTransformer"
      ],
      "metadata": {
        "id": "JslAYQD2bpSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 3B.2 (REVISI TOTAL): Pipeline Preprocessing dengan RobustScaler\n",
        "# =============================================================================\n",
        "print(\"\\n--- SEL 3B.2: Membangun Pipeline Preprocessing yang Robust ---\")\n",
        "\n",
        "# Impor scaler baru\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "if 'X_train' in globals():\n",
        "    try:\n",
        "        numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "        categorical_features = X_train.select_dtypes(include='object').columns.tolist()\n",
        "        print(f\"Ditemukan {len(numeric_features)} fitur numerik dan {len(categorical_features)} fitur kategorikal.\")\n",
        "\n",
        "        # --- Pipeline Numerik dengan RobustScaler ---\n",
        "        numeric_pipeline = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='median', add_indicator=True)),\n",
        "            # GANTI StandardScaler dengan RobustScaler\n",
        "            ('scaler', RobustScaler())\n",
        "        ])\n",
        "        print(\"\\nPipeline untuk fitur numerik dibuat dengan RobustScaler (tahan terhadap outlier).\")\n",
        "\n",
        "        # --- Pipeline Kategorikal (tetap sama) ---\n",
        "        categorical_pipeline = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "        ])\n",
        "        print(\"Pipeline untuk fitur kategorikal dibuat (Imputer + OneHotEncoder).\")\n",
        "\n",
        "        # --- Gabungkan dengan ColumnTransformer (tetap sama) ---\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_pipeline, numeric_features),\n",
        "                ('cat', categorical_pipeline, categorical_features)\n",
        "            ],\n",
        "            remainder='passthrough'\n",
        "        )\n",
        "        print(\"\\n‚úÖ ColumnTransformer berhasil dibuat untuk menggabungkan semua pipeline.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå GAGAL pada SEL 3B.2. Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: DataFrame 'X_train' tidak ditemukan. Jalankan SEL 3B.1 terlebih dahulu.\")\n"
      ],
      "metadata": {
        "id": "4gslW86fblls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 3B.3: Penerapan Pipeline dan Penyimpanan Data yang Telah Diproses"
      ],
      "metadata": {
        "id": "f2at3xZ1b15K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 3B.3 (REVISI): Menerapkan Pipeline dan Menyimpan Hasil\n",
        "# =============================================================================\n",
        "print(\"\\n--- SEL 3B.3: Menerapkan Pipeline dan Menyimpan Hasil ---\")\n",
        "\n",
        "if 'preprocessor' in globals():\n",
        "    try:\n",
        "        print(\"Menerapkan preprocessor pada X_train (fit_transform)...\")\n",
        "        X_train_processed = preprocessor.fit_transform(X_train)\n",
        "        print(f\"   -> X_train_processed dibuat dengan shape: {X_train_processed.shape}\")\n",
        "\n",
        "        print(\"\\nMenerapkan preprocessor pada X_test (transform)...\")\n",
        "        X_test_processed = preprocessor.transform(X_test)\n",
        "        print(f\"   -> X_test_processed dibuat dengan shape: {X_test_processed.shape}\")\n",
        "\n",
        "        print(\"\\nüíæ Menyimpan semua data yang telah diproses dan preprocessor...\")\n",
        "        joblib.dump(preprocessor, os.path.join(MODELS_DIR, 'preprocessor.joblib'))\n",
        "        np.save(os.path.join(DATA_PROCESSED_DIR, 'X_train_processed.npy'), X_train_processed)\n",
        "        np.save(os.path.join(DATA_PROCESSED_DIR, 'X_test_processed.npy'), X_test_processed)\n",
        "        np.save(os.path.join(DATA_PROCESSED_DIR, 'y_train.npy'), y_train.values)\n",
        "        np.save(os.path.join(DATA_PROCESSED_DIR, 'y_test.npy'), y_test.values)\n",
        "\n",
        "        print(\"\\n‚úÖ Semua data telah siap untuk tahap pemodelan (Bagian 4).\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå GAGAL pada SEL 3B.3. Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: Object 'preprocessor' tidak ditemukan. Jalankan SEL 3B.2 terlebih dahulu.\")\n"
      ],
      "metadata": {
        "id": "N8sFz1llb02C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagian 4: Eksperimen Komprehensif"
      ],
      "metadata": {
        "id": "bOndEWbmewbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 4.1: Pemuatan Data & Pengaturan Eksperimen"
      ],
      "metadata": {
        "id": "96CCJTG8e5Bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 4.1: Pemuatan Data & Pengaturan Eksperimen\n",
        "# =============================================================================\n",
        "print(\"--- SEL 4.1: Memuat Data yang Telah Diproses dan Mengatur Eksperimen ---\")\n",
        "\n",
        "# Impor pustaka tambahan yang mungkin diperlukan untuk resampling\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline # Ganti nama agar tidak bentrok dengan pipeline sklearn\n",
        "import time\n",
        "\n",
        "if 'DATA_PROCESSED_DIR' in globals():\n",
        "    try:\n",
        "        # --- 1. Muat Semua Data yang Telah Diproses ---\n",
        "        print(\"üíæ Memuat data .npy dari disk...\")\n",
        "        X_train_processed = np.load(os.path.join(DATA_PROCESSED_DIR, 'X_train_processed.npy'))\n",
        "        X_test_processed = np.load(os.path.join(DATA_PROCESSED_DIR, 'X_test_processed.npy'))\n",
        "        y_train = np.load(os.path.join(DATA_PROCESSED_DIR, 'y_train.npy'))\n",
        "        y_test = np.load(os.path.join(DATA_PROCESSED_DIR, 'y_test.npy'))\n",
        "        print(\"   -> Semua data berhasil dimuat.\")\n",
        "        print(f\"   -> Dimensi: X_train={X_train_processed.shape}, X_test={X_test_processed.shape}\")\n",
        "\n",
        "        # --- 2. Definisikan Model yang Akan Diuji ---\n",
        "        # Kita akan menggunakan parameter dasar (default) untuk perbandingan awal.\n",
        "        # random_state=42 digunakan untuk memastikan hasil yang dapat direproduksi.\n",
        "        models = {\n",
        "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "            'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "            'XGBoost': xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss', n_jobs=-1),\n",
        "            'LightGBM': lgb.LGBMClassifier(random_state=42, n_jobs=-1),\n",
        "            'CatBoost': cb.CatBoostClassifier(random_state=42, verbose=0) # verbose=0 agar tidak ada output saat training\n",
        "        }\n",
        "        print(f\"\\nü§ñ {len(models)} model telah didefinisikan untuk diuji.\")\n",
        "\n",
        "        # --- 3. Definisikan Strategi Penanganan Data Tidak Seimbang ---\n",
        "        # 'None' berarti tidak ada penanganan khusus.\n",
        "        # 'Class Weight' akan ditangani di dalam loop.\n",
        "        # SMOTE dan ADASYN akan diterapkan sebagai langkah resampling.\n",
        "        resampling_strategies = {\n",
        "            'None': None,\n",
        "            'SMOTE': SMOTE(random_state=42),\n",
        "            'ADASYN': ADASYN(random_state=42)\n",
        "        }\n",
        "        # Tambahkan 'Class Weight' sebagai penanda, bukan sebagai objek resampler\n",
        "        all_strategies = ['None', 'Class Weight', 'SMOTE', 'ADASYN']\n",
        "        print(f\"‚öñÔ∏è  {len(all_strategies)} strategi penanganan data tidak seimbang akan diuji.\")\n",
        "\n",
        "        # --- 4. Siapkan Tempat untuk Menyimpan Hasil ---\n",
        "        results_list = []\n",
        "        print(\"\\n‚úÖ Pengaturan eksperimen selesai. Siap untuk memulai perbandingan model.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå ERROR: File .npy tidak ditemukan di {DATA_PROCESSED_DIR}. Harap jalankan Bagian 3B terlebih dahulu.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå GAGAL pada SEL 4.1. Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: Variabel path 'DATA_PROCESSED_DIR' tidak ditemukan.\")\n"
      ],
      "metadata": {
        "id": "TBgmVxrZe356"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 4.2: Eksekusi Eksperimen Perbandingan Model"
      ],
      "metadata": {
        "id": "T7XENzpEfIIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 4.2 (REVISI FINAL): Eksekusi Eksperimen dengan Metrik Tambahan & Penyimpanan\n",
        "# =============================================================================\n",
        "print(\"\\n--- SEL 4.2: Memulai Eksekusi Eksperimen dengan Metrik Tambahan ---\")\n",
        "\n",
        "# Fungsi untuk menghitung statistik KS\n",
        "def calculate_ks(y_true, y_pred_proba):\n",
        "    \"\"\"Menghitung statistik Kolmogorov-Smirnov.\"\"\"\n",
        "    df = pd.DataFrame({'y_true': y_true, 'y_pred_proba': y_pred_proba})\n",
        "    df_sorted = df.sort_values(by='y_pred_proba', ascending=False)\n",
        "\n",
        "    df_sorted['cum_bad'] = df_sorted['y_true'].cumsum() / df_sorted['y_true'].sum()\n",
        "    df_sorted['cum_good'] = (1 - df_sorted['y_true']).cumsum() / (1 - df_sorted['y_true']).sum()\n",
        "\n",
        "    ks_statistic = np.max(np.abs(df_sorted['cum_good'] - df_sorted['cum_bad']))\n",
        "\n",
        "    return ks_statistic\n",
        "\n",
        "if 'models' in globals():\n",
        "    results_list = [] # Mulai ulang daftar hasil\n",
        "\n",
        "    # Loop melalui setiap strategi\n",
        "    for strategy_name in all_strategies:\n",
        "        print(f\"\\n{'='*20} MENJALANKAN STRATEGI: {strategy_name.upper()} {'='*20}\")\n",
        "\n",
        "        # Loop melalui setiap model\n",
        "        for model_name, model in models.items():\n",
        "            start_time = time.time()\n",
        "\n",
        "            X_train_resampled, y_train_resampled = X_train_processed, y_train\n",
        "\n",
        "            # Logika untuk Class Weight dan Resampling\n",
        "            if strategy_name == 'Class Weight':\n",
        "                if hasattr(model, 'class_weight'): model.set_params(class_weight='balanced')\n",
        "                elif hasattr(model, 'scale_pos_weight'):\n",
        "                    scale = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
        "                    model.set_params(scale_pos_weight=scale)\n",
        "            elif strategy_name in ['SMOTE', 'ADASYN']:\n",
        "                print(f\"   -> Menerapkan {strategy_name}...\")\n",
        "                resampler = resampling_strategies[strategy_name]\n",
        "                X_train_resampled, y_train_resampled = resampler.fit_resample(X_train_processed, y_train)\n",
        "\n",
        "            # Latih Model\n",
        "            print(f\"   -> Melatih model: {model_name}...\")\n",
        "            model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "            # Lakukan Prediksi & Evaluasi\n",
        "            y_pred = model.predict(X_test_processed)\n",
        "            y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
        "\n",
        "            # Hitung Semua Metrik\n",
        "            auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "            ks_score = calculate_ks(y_test, y_pred_proba)\n",
        "            gini_score = 2 * auc_score - 1\n",
        "\n",
        "            metrics = {\n",
        "                'Model': model_name,\n",
        "                'Strategy': strategy_name,\n",
        "                'AUC': auc_score,\n",
        "                'Gini': gini_score,\n",
        "                'KS': ks_score,\n",
        "                'F1-Score': f1_score(y_test, y_pred),\n",
        "                'Recall': recall_score(y_test, y_pred),\n",
        "                'Precision': precision_score(y_test, y_pred),\n",
        "                'Accuracy': accuracy_score(y_test, y_pred),\n",
        "                'Training Time (s)': time.time() - start_time\n",
        "            }\n",
        "\n",
        "            results_list.append(metrics)\n",
        "            print(f\"      -> Selesai. AUC: {metrics['AUC']:.4f}, Gini: {metrics['Gini']:.4f}, KS: {metrics['KS']:.4f}\")\n",
        "\n",
        "    # --- Konversi Hasil ke DataFrame ---\n",
        "    results_df_comprehensive = pd.DataFrame(results_list)\n",
        "    print(\"\\n\\n‚úÖ SEMUA EKSPERIMEN SELESAI.\")\n",
        "    print(\"Hasil perbandingan model komprehensif tersimpan dalam DataFrame 'results_df_comprehensive'.\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # --- BLOK BARU: Simpan Hasil ke File CSV ---\n",
        "    # =========================================================================\n",
        "    if 'RESULTS_DIR' in globals():\n",
        "        try:\n",
        "            # Buat subfolder jika belum ada\n",
        "            analysis_data_dir = os.path.join(RESULTS_DIR, 'analysis_data')\n",
        "            os.makedirs(analysis_data_dir, exist_ok=True)\n",
        "\n",
        "            # Tentukan path file\n",
        "            results_file_path = os.path.join(analysis_data_dir, 'model_comparison_results.csv')\n",
        "\n",
        "            # Simpan DataFrame ke CSV\n",
        "            results_df_comprehensive.to_csv(results_file_path, index=False)\n",
        "\n",
        "            print(f\"\\nüíæ Hasil eksperimen berhasil disimpan ke: {results_file_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå GAGAL menyimpan hasil eksperimen. Error: {e}\")\n",
        "    # =========================================================================\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå ERROR: Objek 'models' tidak ditemukan. Jalankan SEL 4.1 terlebih dahulu.\")\n"
      ],
      "metadata": {
        "id": "SLI5FYpvfHCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 4.3 (BARU): Analisis Tabel Perbandingan Komprehensif"
      ],
      "metadata": {
        "id": "M8VDN3rqzJol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 4.3 (BARU): Analisis Tabel Perbandingan Komprehensif\n",
        "# =============================================================================\n",
        "print(\"--- SEL 4.3: Menganalisis Hasil Perbandingan Model Komprehensif ---\")\n",
        "\n",
        "if 'results_df_comprehensive' in globals():\n",
        "    # --- 1. Tampilkan Tabel Hasil Lengkap ---\n",
        "    # Urutkan berdasarkan metrik yang paling penting untuk pemisahan, misal Gini atau KS.\n",
        "    print(\"\\n--- Tabel Perbandingan Model (Diurutkan berdasarkan Gini) ---\")\n",
        "    display(results_df_comprehensive.sort_values(by='Gini', ascending=False))\n",
        "\n",
        "    print(\"\\n--- Tabel Perbandingan Model (Diurutkan berdasarkan KS) ---\")\n",
        "    display(results_df_comprehensive.sort_values(by='KS', ascending=False))\n",
        "\n",
        "    # --- 2. Visualisasi Perbandingan Metrik Kunci ---\n",
        "    # Fokus pada metrik pemisahan (AUC, Gini, KS)\n",
        "    metrics_to_plot = ['AUC', 'Gini', 'KS']\n",
        "\n",
        "    # Ambil 10 kombinasi model/strategi terbaik berdasarkan AUC\n",
        "    top_10_results = results_df_comprehensive.sort_values(by='AUC', ascending=False).head(10)\n",
        "    top_10_results['Model_Strategy'] = top_10_results['Model'] + ' (' + top_10_results['Strategy'] + ')'\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    # Lelehkan dataframe untuk format yang ramah untuk seaborn\n",
        "    df_melted = top_10_results.melt(\n",
        "        id_vars='Model_Strategy',\n",
        "        value_vars=metrics_to_plot,\n",
        "        var_name='Metric',\n",
        "        value_name='Score'\n",
        "    )\n",
        "\n",
        "    sns.barplot(data=df_melted, x='Score', y='Model_Strategy', hue='Metric', palette='viridis')\n",
        "    plt.title('Perbandingan Metrik Pemisahan untuk 10 Model Terbaik')\n",
        "    plt.xlabel('Score')\n",
        "    plt.ylabel('Model (Strategy)')\n",
        "    plt.legend(title='Metric')\n",
        "    plt.grid(axis='x')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå ERROR: DataFrame 'results_df_comprehensive' tidak ditemukan. Jalankan SEL 4.2 yang direvisi terlebih dahulu.\")\n"
      ],
      "metadata": {
        "id": "0oMN9TSbzHuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 4.4: Investigasi Threshold Optimal (Multi-Strategi)"
      ],
      "metadata": {
        "id": "RRiKJK9_0SFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 4.4: Investigasi Threshold Optimal (Multi-Strategi)\n",
        "# =============================================================================\n",
        "print(\"\\n--- SEL 4.2.1+: Investigasi Threshold Optimal (Multi-Strategi) ---\")\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    f1_score, recall_score, precision_score,\n",
        "    precision_recall_curve, roc_curve, roc_auc_score, confusion_matrix\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 1. Pilih Model & Strategi Terbaik ---\n",
        "best_model_name = 'XGBoost'\n",
        "best_strategy_name = 'Class Weight'\n",
        "print(f\"Model terbaik: {best_model_name} | Strategi: {best_strategy_name}\")\n",
        "\n",
        "# --- 2. Buat Validation Split dari Training ---\n",
        "X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
        "    X_train_processed, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "# --- 3. Latih Model pada Train_sub ---\n",
        "scale = len(y_train_sub[y_train_sub == 0]) / len(y_train_sub[y_train_sub == 1])\n",
        "model_to_tune = xgb.XGBClassifier(\n",
        "    random_state=42, use_label_encoder=False, eval_metric='logloss',\n",
        "    n_jobs=-1, scale_pos_weight=scale\n",
        ")\n",
        "\n",
        "print(\"Melatih ulang model terbaik pada subset train...\")\n",
        "model_to_tune.fit(X_train_sub, y_train_sub)\n",
        "\n",
        "# --- 4. Prediksi Probabilitas ---\n",
        "y_val_proba = model_to_tune.predict_proba(X_val)[:, 1]\n",
        "y_test_proba = model_to_tune.predict_proba(X_test_processed)[:, 1]\n",
        "\n",
        "# ========================================================================\n",
        "# üîπ Metode 1: Grid Search F1 (manual loop)\n",
        "# ========================================================================\n",
        "thresholds = np.arange(0.01, 1.0, 0.01)\n",
        "f1_scores = [f1_score(y_val, (y_val_proba >= t).astype(int)) for t in thresholds]\n",
        "\n",
        "opt_f1_idx = np.argmax(f1_scores)\n",
        "opt_f1_th = thresholds[opt_f1_idx]\n",
        "\n",
        "# ========================================================================\n",
        "# üîπ Metode 2: Dari Precision-Recall Curve\n",
        "# ========================================================================\n",
        "precisions, recalls, ths_pr = precision_recall_curve(y_val, y_val_proba)\n",
        "f1_scores_pr = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
        "opt_pr_idx = np.nanargmax(f1_scores_pr)\n",
        "opt_pr_th = ths_pr[opt_pr_idx]\n",
        "\n",
        "# ========================================================================\n",
        "# üîπ Metode 3: Dari ROC Curve (Youden‚Äôs J = TPR - FPR)\n",
        "# ========================================================================\n",
        "fpr, tpr, ths_roc = roc_curve(y_val, y_val_proba)\n",
        "j_scores = tpr - fpr\n",
        "opt_roc_idx = np.argmax(j_scores)\n",
        "opt_roc_th = ths_roc[opt_roc_idx]\n",
        "\n",
        "# ========================================================================\n",
        "# üîπ Evaluasi semua threshold di Test Set\n",
        "# ========================================================================\n",
        "def evaluate_preds(y_true, y_proba, threshold, label=\"\"):\n",
        "    y_pred = (y_proba >= threshold).astype(int)\n",
        "    return {\n",
        "        \"Label\": label,\n",
        "        \"Threshold\": threshold,\n",
        "        \"F1\": f1_score(y_true, y_pred),\n",
        "        \"Recall\": recall_score(y_true, y_pred),\n",
        "        \"Precision\": precision_score(y_true, y_pred),\n",
        "        \"AUC\": roc_auc_score(y_true, y_proba)\n",
        "    }\n",
        "\n",
        "results_compare = pd.DataFrame([\n",
        "    evaluate_preds(y_test, y_test_proba, 0.5, \"Default (0.5)\"),\n",
        "    evaluate_preds(y_test, y_test_proba, opt_f1_th, f\"F1-Grid ({opt_f1_th:.2f})\"),\n",
        "    evaluate_preds(y_test, y_test_proba, opt_pr_th, f\"PR-Curve ({opt_pr_th:.2f})\"),\n",
        "    evaluate_preds(y_test, y_test_proba, opt_roc_th, f\"ROC-Youden ({opt_roc_th:.2f})\"),\n",
        "])\n",
        "\n",
        "display(results_compare)\n",
        "\n",
        "# ========================================================================\n",
        "# üîπ Visualisasi Threshold vs F1\n",
        "# ========================================================================\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(thresholds, f1_scores, marker='o', linestyle='--', label=\"Grid F1\")\n",
        "plt.axvline(opt_f1_th, color='r', linestyle='--', label=f\"F1 Opt = {opt_f1_th:.2f}\")\n",
        "plt.axvline(opt_pr_th, color='g', linestyle='--', label=f\"PR Opt = {opt_pr_th:.2f}\")\n",
        "plt.axvline(opt_roc_th, color='b', linestyle='--', label=f\"ROC Opt = {opt_roc_th:.2f}\")\n",
        "plt.title(\"Threshold vs F1-Score (Validation Set)\")\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.ylabel(\"F1-Score\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ========================================================================\n",
        "# üîπ Precision-Recall Curve\n",
        "# ========================================================================\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(recalls, precisions, label=\"Precision-Recall Curve\")\n",
        "plt.axvline(x=recall_score(y_test, (y_test_proba >= opt_pr_th).astype(int)),\n",
        "            color=\"g\", linestyle=\"--\", label=f\"PR Opt Th={opt_pr_th:.2f}\")\n",
        "plt.title(\"Precision-Recall Curve (Test Set)\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HlKSur7CrZCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 4.5: Hyperparameter Tuning untuk Model-model Terbaik"
      ],
      "metadata": {
        "id": "tF4JgVwQS_9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 4.5 (FINAL & ROBUST): Hyperparameter Tuning & Penyimpanan Semua Model\n",
        "# =============================================================================\n",
        "print(\"--- SEL 4.5 (FINAL): Memulai Hyperparameter Tuning & Penyimpanan Semua Model ---\")\n",
        "\n",
        "# Impor pustaka yang diperlukan\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform, randint\n",
        "import time\n",
        "import json # Untuk menyimpan parameter dalam format yang rapi\n",
        "\n",
        "# Pastikan data yang telah diproses sudah ada\n",
        "if 'X_train_processed' in globals():\n",
        "\n",
        "    # --- 1. Definisikan Ruang Parameter yang Dioptimalkan ---\n",
        "    param_grids = {\n",
        "        'Logistic Regression': {\n",
        "            'C': uniform(0.01, 10),\n",
        "            'solver': ['liblinear']\n",
        "        },\n",
        "        'LightGBM': {\n",
        "            'n_estimators': randint(100, 800),\n",
        "            'learning_rate': uniform(0.01, 0.1),\n",
        "            'num_leaves': randint(20, 60),\n",
        "            'max_depth': [-1, 5, 10, 15],\n",
        "            'subsample': uniform(0.6, 0.4),\n",
        "            'colsample_bytree': uniform(0.6, 0.4)\n",
        "        },\n",
        "        'XGBoost': {\n",
        "            'n_estimators': randint(100, 800),\n",
        "            'learning_rate': uniform(0.01, 0.1),\n",
        "            'max_depth': randint(3, 10),\n",
        "            'subsample': uniform(0.6, 0.4),\n",
        "            'colsample_bytree': uniform(0.6, 0.4),\n",
        "            'gamma': [0, 1, 5]\n",
        "        },\n",
        "        'CatBoost': {\n",
        "            'iterations': randint(100, 800),\n",
        "            'learning_rate': uniform(0.01, 0.1),\n",
        "            'depth': randint(4, 10),\n",
        "            'l2_leaf_reg': uniform(1, 10)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # --- 2. Siapkan Model ---\n",
        "    scale = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
        "    models_to_tune = {\n",
        "        #'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),\n",
        "        #'LightGBM': lgb.LGBMClassifier(random_state=42, n_jobs=-1, class_weight='balanced'),\n",
        "        #'XGBoost': xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss', n_jobs=-1, scale_pos_weight=scale),\n",
        "        'CatBoost': cb.CatBoostClassifier(random_state=42, verbose=0, auto_class_weights='Balanced')\n",
        "    }\n",
        "\n",
        "    # --- 3. Jalankan Proses Tuning ---\n",
        "    tuning_results = []\n",
        "    best_estimators = {} # Dictionary untuk menyimpan model terbaik dari setiap jenis\n",
        "\n",
        "    for model_name, model in models_to_tune.items():\n",
        "        print(f\"\\n{'='*25}\\nüöÄ TUNING MODEL: {model_name}\\n{'='*25}\")\n",
        "\n",
        "        random_search = RandomizedSearchCV(\n",
        "            estimator=model,\n",
        "            param_distributions=param_grids[model_name],\n",
        "            n_iter=25,\n",
        "            cv=3,\n",
        "            scoring='f1',\n",
        "            n_jobs=-1,\n",
        "            random_state=42,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"Memulai Randomized Search...\")\n",
        "        random_search.fit(X_train_processed, y_train)\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Simpan hasil tuning\n",
        "        tuning_results.append({\n",
        "            'Model': model_name,\n",
        "            'Best Score (CV F1)': random_search.best_score_,\n",
        "            'Best Params': random_search.best_params_,\n",
        "            'Tuning Time (m)': (end_time - start_time) / 60\n",
        "        })\n",
        "\n",
        "        # Simpan model terbaik ke dalam dictionary\n",
        "        best_estimators[model_name] = random_search.best_estimator_\n",
        "\n",
        "        print(f\"‚úÖ Tuning untuk {model_name} selesai dalam {(end_time - start_time) / 60:.2f} menit.\")\n",
        "        print(f\"   -> Skor F1 CV Terbaik: {random_search.best_score_:.4f}\")\n",
        "\n",
        "    # --- 4. Tampilkan Ringkasan Hasil Tuning ---\n",
        "    print(\"\\n\\n\" + \"=\"*60)\n",
        "    print(\"      RINGKASAN HASIL HYPERPARAMETER TUNING\")\n",
        "    print(\"=\"*60)\n",
        "    tuning_results_df = pd.DataFrame(tuning_results)\n",
        "    display(tuning_results_df.sort_values(by='Best Score (CV F1)', ascending=False))\n",
        "\n",
        "    # =========================================================================\n",
        "    # --- BAGIAN BARU: Simpan SEMUA Model yang Telah Di-tuning ---\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"      üíæ MENYIMPAN SEMUA MODEL TERBAIK\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if 'MODELS_DIR' in globals():\n",
        "        # Loop melalui dictionary 'best_estimators' yang sudah kita isi\n",
        "        for model_name, model_object in best_estimators.items():\n",
        "            # Buat nama file yang bersih (misal, 'Logistic Regression' -> 'Logistic_Regression')\n",
        "            safe_model_name = model_name.replace(\" \", \"_\")\n",
        "            model_path = os.path.join(MODELS_DIR, f'tuned_model_{safe_model_name}.joblib')\n",
        "\n",
        "            # Simpan model\n",
        "            joblib.dump(model_object, model_path)\n",
        "            print(f\"   -> ‚úÖ Model '{model_name}' berhasil disimpan di: {model_path}\")\n",
        "\n",
        "        # Juga simpan DataFrame hasil tuning untuk referensi nanti\n",
        "        tuning_results_df.to_csv(os.path.join(RESULTS_DIR, 'analysis_data', 'tuning_summary.csv'), index=False)\n",
        "        print(\"\\n   -> ‚úÖ Ringkasan hasil tuning juga disimpan dalam file CSV.\")\n",
        "\n",
        "    print(\"\\nSemua model telah disimpan. Anda bisa me-restart sesi tanpa kehilangan hasil tuning.\")\n",
        "    print(\"=\"*60)\n",
        "    # =========================================================================\n",
        "\n",
        "    # --- 5. Evaluasi Cepat Model Juara (Opsional, karena sudah disimpan) ---\n",
        "    champion_model_name = tuning_results_df.sort_values(by='Best Score (CV F1)', ascending=False).iloc[0]['Model']\n",
        "    champion_model = best_estimators[champion_model_name]\n",
        "\n",
        "    print(f\"\\n\\n--- Laporan Klasifikasi Cepat untuk Model Juara ({champion_model_name}) pada Data Uji ---\")\n",
        "    y_pred_test = champion_model.predict(X_test_processed)\n",
        "    print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred_test)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=['Good', 'Bad'], yticklabels=['Good', 'Bad'])\n",
        "    plt.title(f'Confusion Matrix - {champion_model_name} (Tuned)')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå ERROR: Data yang telah diproses tidak ditemukan. Jalankan Bagian 3B terlebih dahulu.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "poPfIeu7S-Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 4.5: Perbandingan Confusion Matrix untuk Model yang Telah Di-tuning\n",
        "# =============================================================================\n",
        "print(\"--- SEL 4.5: Memulai Perbandingan Confusion Matrix ---\")\n",
        "\n",
        "# Pastikan hasil tuning sudah ada\n",
        "if 'best_estimators' in globals():\n",
        "    try:\n",
        "        # Tentukan jumlah baris dan kolom untuk subplot\n",
        "        n_models = len(best_estimators)\n",
        "        n_cols = 2\n",
        "        n_rows = (n_models + 1) // n_cols\n",
        "\n",
        "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 5))\n",
        "        axes = axes.flatten() # Ubah menjadi array 1D agar mudah di-loop\n",
        "\n",
        "        # Loop melalui setiap model terbaik\n",
        "        for i, (model_name, model) in enumerate(best_estimators.items()):\n",
        "            print(f\"   -> Membuat Confusion Matrix untuk: {model_name}\")\n",
        "\n",
        "            # Lakukan prediksi pada data uji\n",
        "            y_pred = model.predict(X_test_processed)\n",
        "\n",
        "            # Buat confusion matrix\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "            # Plot heatmap\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
        "                        xticklabels=['Good Loan', 'Bad Loan'],\n",
        "                        yticklabels=['Good Loan', 'Bad Loan'])\n",
        "\n",
        "            axes[i].set_title(f'Confusion Matrix - {model_name}')\n",
        "            axes[i].set_xlabel('Predicted Label')\n",
        "            axes[i].set_ylabel('True Label')\n",
        "\n",
        "        # Sembunyikan sumbu yang tidak digunakan jika jumlah model ganjil\n",
        "        for j in range(i + 1, len(axes)):\n",
        "            axes[j].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå GAGAL pada SEL 4.5. Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: Objek 'best_estimators' tidak ditemukan. Jalankan SEL 4.4 terlebih dahulu.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9XOVboDIflvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 4.6: Perbandingan Learning Curve untuk Model yang Telah Di-tuning"
      ],
      "metadata": {
        "id": "0J6k8lLaQ0Jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 4.6: Perbandingan Learning Curve untuk Model yang Telah Di-tuning\n",
        "# =============================================================================\n",
        "print(\"\\n--- SEL 4.6: Memulai Perbandingan Learning Curve ---\")\n",
        "\n",
        "# Impor pustaka yang diperlukan\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "# Pastikan hasil tuning sudah ada\n",
        "if 'best_estimators' in globals():\n",
        "    try:\n",
        "        # Tentukan jumlah baris dan kolom untuk subplot\n",
        "        n_models = len(best_estimators)\n",
        "        n_cols = 2\n",
        "        n_rows = (n_models + 1) // n_cols\n",
        "\n",
        "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 8, n_rows * 6), sharey=True)\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        # Loop melalui setiap model terbaik\n",
        "        for i, (model_name, model) in enumerate(best_estimators.items()):\n",
        "            print(f\"   -> Membuat Learning Curve untuk: {model_name}...\")\n",
        "\n",
        "            # Hasilkan data untuk learning curve\n",
        "            # cv=3: menggunakan 3-fold cross-validation\n",
        "            # scoring='f1': metrik yang kita pedulikan\n",
        "            # n_jobs=-1: gunakan semua core CPU\n",
        "            train_sizes, train_scores, validation_scores = learning_curve(\n",
        "                estimator=model,\n",
        "                X=X_train_processed,\n",
        "                y=y_train,\n",
        "                cv=3,\n",
        "                scoring='f1',\n",
        "                n_jobs=-1,\n",
        "                train_sizes=np.linspace(0.1, 1.0, 10), # Gunakan 10 titik ukuran data\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            # Hitung rata-rata dan standar deviasi\n",
        "            train_scores_mean = np.mean(train_scores, axis=1)\n",
        "            train_scores_std = np.std(train_scores, axis=1)\n",
        "            validation_scores_mean = np.mean(validation_scores, axis=1)\n",
        "            validation_scores_std = np.std(validation_scores, axis=1)\n",
        "\n",
        "            # Plot kurva\n",
        "            ax = axes[i]\n",
        "            ax.plot(train_sizes, train_scores_mean, 'o-', color='blue', label='Training score')\n",
        "            ax.plot(train_sizes, validation_scores_mean, 'o-', color='green', label='Cross-validation score')\n",
        "\n",
        "            # Plot area standar deviasi\n",
        "            ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                            train_scores_mean + train_scores_std, alpha=0.1, color='blue')\n",
        "            ax.fill_between(train_sizes, validation_scores_mean - validation_scores_std,\n",
        "                            validation_scores_mean + validation_scores_std, alpha=0.1, color='green')\n",
        "\n",
        "            ax.set_title(f'Learning Curve - {model_name}')\n",
        "            ax.set_xlabel('Training examples')\n",
        "            ax.set_ylabel('F1 Score')\n",
        "            ax.legend(loc='best')\n",
        "            ax.grid(True)\n",
        "\n",
        "        # Sembunyikan sumbu yang tidak digunakan\n",
        "        for j in range(i + 1, len(axes)):\n",
        "            axes[j].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå GAGAL pada SEL 4.6. Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: Objek 'best_estimators' tidak ditemukan. Jalankan SEL 4.4 terlebih dahulu.\")"
      ],
      "metadata": {
        "id": "Bxow4g2lQy6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 4.7 (REVISI LENGKAP): Evaluasi Metrik Komprehensif dari Model Tuned"
      ],
      "metadata": {
        "id": "ieHrDntAQ2Nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 4.7 (REVISI LENGKAP): Evaluasi Metrik Komprehensif dari Model Tuned\n",
        "# =============================================================================\n",
        "print(\"--- SEL 4.7: Memulai Evaluasi Metrik Lengkap dari Semua Model yang Telah Di-tuning ---\")\n",
        "\n",
        "# --- Fungsi Helper untuk Menghitung KS ---\n",
        "def calculate_ks(y_true, y_pred_proba):\n",
        "    \"\"\"Menghitung statistik Kolmogorov-Smirnov.\"\"\"\n",
        "    df = pd.DataFrame({'y_true': y_true, 'y_pred_proba': y_pred_proba})\n",
        "    df_sorted = df.sort_values(by='y_pred_proba', ascending=False)\n",
        "\n",
        "    # Hitung distribusi kumulatif untuk kelas 'good' (0) dan 'bad' (1)\n",
        "    df_sorted['cum_bad'] = df_sorted['y_true'].cumsum() / df_sorted['y_true'].sum()\n",
        "    df_sorted['cum_good'] = (1 - df_sorted['y_true']).cumsum() / (1 - df_sorted['y_true']).sum()\n",
        "\n",
        "    # KS adalah perbedaan maksimum antara dua distribusi kumulatif\n",
        "    ks_statistic = np.max(np.abs(df_sorted['cum_good'] - df_sorted['cum_bad']))\n",
        "\n",
        "    return ks_statistic\n",
        "\n",
        "try:\n",
        "    # --- 1. Pastikan Data Uji Tersedia ---\n",
        "    if 'X_test_processed' not in globals() or 'y_test' not in globals():\n",
        "        print(\"‚ö†Ô∏è  Data uji tidak ditemukan di memori, memuat ulang dari file...\")\n",
        "        X_test_processed = np.load(os.path.join(DATA_PROCESSED_DIR, 'X_test_processed.npy'))\n",
        "        y_test = np.load(os.path.join(DATA_PROCESSED_DIR, 'y_test.npy'))\n",
        "        print(\"   -> Data uji berhasil dimuat.\")\n",
        "\n",
        "    # --- 2. Loop Melalui Semua Model Tersimpan ---\n",
        "    model_names = ['Logistic Regression', 'LightGBM', 'XGBoost', 'CatBoost']\n",
        "    evaluation_results = []\n",
        "\n",
        "    print(\"\\nüìä Mengevaluasi setiap model yang telah di-tuning pada data uji...\")\n",
        "    for model_name in model_names:\n",
        "        safe_model_name = model_name.replace(\" \", \"_\")\n",
        "        # Sesuaikan path untuk CatBoost jika namanya berbeda\n",
        "        if model_name == 'CatBoost' and not os.path.exists(os.path.join(MODELS_DIR, f'tuned_model_{safe_model_name}.joblib')):\n",
        "             model_path = os.path.join(MODELS_DIR, f'champion_model_{safe_model_name}.joblib')\n",
        "        else:\n",
        "             model_path = os.path.join(MODELS_DIR, f'tuned_model_{safe_model_name}.joblib')\n",
        "\n",
        "        try:\n",
        "            # Muat model\n",
        "            model = joblib.load(model_path)\n",
        "\n",
        "            # Lakukan prediksi (ini cepat)\n",
        "            y_pred = model.predict(X_test_processed)\n",
        "            y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
        "\n",
        "            # --- 3. Hitung SEMUA Metrik ---\n",
        "            auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "            metrics = {\n",
        "                'Model': model_name,\n",
        "                'AUC': auc_score,\n",
        "                'Gini': 2 * auc_score - 1, # Hitung Gini dari AUC\n",
        "                'KS': calculate_ks(y_test, y_pred_proba), # Hitung KS\n",
        "                'F1-Score': f1_score(y_test, y_pred),\n",
        "                'Recall': recall_score(y_test, y_pred),\n",
        "                'Precision': precision_score(y_test, y_pred),\n",
        "                'Accuracy': accuracy_score(y_test, y_pred)\n",
        "            }\n",
        "            evaluation_results.append(metrics)\n",
        "            print(f\"   -> Metrik untuk '{model_name}' berhasil dihitung.\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"   -> ‚ùå PERINGATAN: File model untuk '{model_name}' tidak ditemukan. Model ini akan dilewati.\")\n",
        "            continue\n",
        "\n",
        "    # --- 4. Tampilkan Hasil dalam Tabel Perbandingan ---\n",
        "    if not evaluation_results:\n",
        "        print(\"\\n‚ùå ERROR: Tidak ada model yang berhasil dievaluasi.\")\n",
        "    else:\n",
        "        df_metrics = pd.DataFrame(evaluation_results)\n",
        "\n",
        "        # Tampilkan dua tabel: satu diurutkan berdasarkan Gini, satu berdasarkan F1-Score\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"      TABEL PERBANDINGAN METRIK (Diurutkan berdasarkan Gini)\")\n",
        "        print(\"=\"*80)\n",
        "        display(df_metrics.sort_values(by='Gini', ascending=False))\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"      TABEL PERBANDINGAN METRIK (Diurutkan berdasarkan F1-Score)\")\n",
        "        print(\"=\"*80)\n",
        "        display(df_metrics.sort_values(by='F1-Score', ascending=False))\n",
        "        print(\"=\"*80)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå GAGAL pada SEL 4.7. Error: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "wLQ5rQDtlIbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 4.8 (BARU): Visualisasi Perbandingan Confusion Matrix Multi-Model"
      ],
      "metadata": {
        "id": "PzS_EbNnQ5HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 4.8 (BARU): Visualisasi Perbandingan Confusion Matrix Multi-Model\n",
        "# =============================================================================\n",
        "print(\"--- SEL 4.8: Memulai Visualisasi Perbandingan Confusion Matrix ---\")\n",
        "\n",
        "try:\n",
        "    # Pastikan data uji sudah ada di memori\n",
        "    if 'X_test_processed' not in globals() or 'y_test' not in globals():\n",
        "        print(\"‚ö†Ô∏è  Data uji tidak ditemukan, memuat ulang dari file...\")\n",
        "        X_test_processed = np.load(os.path.join(DATA_PROCESSED_DIR, 'X_test_processed.npy'))\n",
        "        y_test = np.load(os.path.join(DATA_PROCESSED_DIR, 'y_test.npy'))\n",
        "        print(\"   -> Data uji berhasil dimuat.\")\n",
        "\n",
        "    # --- 1. Siapkan Model dan Subplot ---\n",
        "    model_names = ['Logistic Regression', 'LightGBM', 'XGBoost', 'CatBoost']\n",
        "\n",
        "    # Buat layout subplot 2x2\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "    # 'Flatten' array axes agar mudah di-loop (dari 2D menjadi 1D)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    print(\"\\nüñºÔ∏è  Membuat plot Confusion Matrix untuk setiap model...\")\n",
        "\n",
        "    # --- 2. Loop, Muat Model, dan Buat Plot ---\n",
        "    for i, model_name in enumerate(model_names):\n",
        "        safe_model_name = model_name.replace(\" \", \"_\")\n",
        "        # Sesuaikan path untuk CatBoost jika namanya berbeda\n",
        "        if model_name == 'CatBoost' and not os.path.exists(os.path.join(MODELS_DIR, f'tuned_model_{safe_model_name}.joblib')):\n",
        "             model_path = os.path.join(MODELS_DIR, f'champion_model_{safe_model_name}.joblib')\n",
        "        else:\n",
        "             model_path = os.path.join(MODELS_DIR, f'tuned_model_{safe_model_name}.joblib')\n",
        "\n",
        "        try:\n",
        "            # Muat model\n",
        "            model = joblib.load(model_path)\n",
        "\n",
        "            # Buat prediksi\n",
        "            y_pred = model.predict(X_test_processed)\n",
        "\n",
        "            # Hitung confusion matrix\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "            # --- Buat Plot Heatmap ---\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
        "                        xticklabels=['Good Loan', 'Bad Loan'],\n",
        "                        yticklabels=['Good Loan', 'Bad Loan'])\n",
        "\n",
        "            axes[i].set_title(f'Confusion Matrix - {model_name}', fontsize=14)\n",
        "            axes[i].set_xlabel('Predicted Label', fontsize=12)\n",
        "            axes[i].set_ylabel('True Label', fontsize=12)\n",
        "            print(f\"   -> Plot untuk '{model_name}' berhasil dibuat.\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"   -> ‚ùå PERINGATAN: File model untuk '{model_name}' tidak ditemukan. Plot ini akan kosong.\")\n",
        "            axes[i].set_title(f'Model {model_name} Tidak Ditemukan')\n",
        "            axes[i].axis('off') # Sembunyikan sumbu jika model tidak ada\n",
        "            continue\n",
        "\n",
        "    # --- 3. Tampilkan Plot ---\n",
        "    plt.tight_layout(pad=3.0) # Beri sedikit ruang antar plot\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå GAGAL pada SEL 4.8. Error: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "aTQIQvBOmLBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 4.9 (BARU): Perbandingan Laporan Klasifikasi Multi-Model"
      ],
      "metadata": {
        "id": "ouyBLYqVQ7YT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 4.9 (BARU): Perbandingan Laporan Klasifikasi Multi-Model\n",
        "# =============================================================================\n",
        "print(\"--- SEL 4.9: Memulai Perbandingan Laporan Klasifikasi untuk Semua Model ---\")\n",
        "\n",
        "try:\n",
        "    # Pastikan data uji sudah ada di memori\n",
        "    if 'X_test_processed' not in globals() or 'y_test' not in globals():\n",
        "        print(\"‚ö†Ô∏è  Data uji tidak ditemukan, memuat ulang dari file...\")\n",
        "        X_test_processed = np.load(os.path.join(DATA_PROCESSED_DIR, 'X_test_processed.npy'))\n",
        "        y_test = np.load(os.path.join(DATA_PROCESSED_DIR, 'y_test.npy'))\n",
        "        print(\"   -> Data uji berhasil dimuat.\")\n",
        "\n",
        "    # --- 1. Siapkan Model ---\n",
        "    model_names = ['Logistic Regression', 'LightGBM', 'XGBoost', 'CatBoost']\n",
        "\n",
        "    print(\"\\n\" + \"=\"*65)\n",
        "    print(\"      LAPORAN KLASIFIKASI UNTUK SETIAP MODEL (PADA DATA UJI)\")\n",
        "    print(\"=\"*65)\n",
        "\n",
        "    # --- 2. Loop, Muat Model, dan Tampilkan Laporan ---\n",
        "    for model_name in model_names:\n",
        "        safe_model_name = model_name.replace(\" \", \"_\")\n",
        "        # Sesuaikan path untuk CatBoost jika namanya berbeda\n",
        "        if model_name == 'CatBoost' and not os.path.exists(os.path.join(MODELS_DIR, f'tuned_model_{safe_model_name}.joblib')):\n",
        "             model_path = os.path.join(MODELS_DIR, f'champion_model_{safe_model_name}.joblib')\n",
        "        else:\n",
        "             model_path = os.path.join(MODELS_DIR, f'tuned_model_{safe_model_name}.joblib')\n",
        "\n",
        "        try:\n",
        "            # Muat model\n",
        "            model = joblib.load(model_path)\n",
        "\n",
        "            # Buat prediksi\n",
        "            y_pred = model.predict(X_test_processed)\n",
        "\n",
        "            # --- Cetak Laporan Klasifikasi ---\n",
        "            print(f\"\\n\\n--- Laporan untuk Model: {model_name} ---\")\n",
        "\n",
        "            # Gunakan target_names untuk label yang lebih deskriptif\n",
        "            report = classification_report(y_test, y_pred, target_names=['Good Loan (0)', 'Bad Loan (1)'])\n",
        "            print(report)\n",
        "            print(\"-\" * 65)\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"\\n\\n--- Laporan untuk Model: {model_name} ---\")\n",
        "            print(f\"   -> ‚ùå PERINGATAN: File model tidak ditemukan. Laporan tidak dapat dibuat.\")\n",
        "            print(\"-\" * 65)\n",
        "            continue\n",
        "\n",
        "    print(\"\\n‚úÖ Semua laporan klasifikasi telah ditampilkan.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå GAGAL pada SEL 4.9. Error: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jKA5FazTmM5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagian 5: Interpretasi Model\n"
      ],
      "metadata": {
        "id": "ueFysetbenMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 5.1 (DINAMIS): Pemuatan Model Pilihan & Interpretasi Global SHAP"
      ],
      "metadata": {
        "id": "EO2JHobiQ-6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 5.1 (DINAMIS): Pemuatan Model Pilihan & Interpretasi Global SHAP\n",
        "# =============================================================================\n",
        "# --- UBAH VARIABEL INI UNTUK MENGANALISIS MODEL YANG BERBEDA ---\n",
        "# Pilihan yang tersedia: 'Logistic Regression', 'LightGBM', 'XGBoost', 'CatBoost'\n",
        "\n",
        "MODEL_TO_ANALYZE = 'CatBoost'\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "print(f\"‚úÖ Konfigurasi diatur untuk menganalisis model: '{MODEL_TO_ANALYZE}'\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\n--- SEL 5.1: Memuat '{MODEL_TO_ANALYZE}' dan Memulai Interpretasi Global SHAP ---\")\n",
        "\n",
        "try:\n",
        "    # --- 1. Tentukan dan Muat Model yang Dipilih ---\n",
        "    safe_model_name = MODEL_TO_ANALYZE.replace(\" \", \"_\")\n",
        "    model_path = os.path.join(MODELS_DIR, f'tuned_model_{safe_model_name}.joblib')\n",
        "\n",
        "    print(f\"üíæ Memuat model '{MODEL_TO_ANALYZE}' dari: {model_path}\")\n",
        "    # Kita gunakan nama variabel 'selected_model' agar lebih jelas\n",
        "    selected_model = joblib.load(model_path)\n",
        "    print(\"   -> Model berhasil dimuat.\")\n",
        "\n",
        "    # --- 2. Muat Preprocessor dan Data Uji ---\n",
        "    preprocessor_path = os.path.join(MODELS_DIR, 'preprocessor.joblib')\n",
        "    preprocessor = joblib.load(preprocessor_path)\n",
        "    print(\"   -> Preprocessor berhasil dimuat.\")\n",
        "\n",
        "    if 'X_test_processed' not in globals():\n",
        "        print(\"   -> Data uji tidak ditemukan, memuat ulang...\")\n",
        "        X_test_processed = np.load(os.path.join(DATA_PROCESSED_DIR, 'X_test_processed.npy'))\n",
        "        y_test = np.load(os.path.join(DATA_PROCESSED_DIR, 'y_test.npy'))\n",
        "\n",
        "    # --- 3. Buat SHAP Explainer ---\n",
        "    print(f\"\\nMembuat SHAP Explainer untuk model {MODEL_TO_ANALYZE}...\")\n",
        "    # Gunakan try-except untuk menangani model linear vs. tree\n",
        "    try:\n",
        "        # Untuk model berbasis pohon (XGBoost, LightGBM, CatBoost)\n",
        "        explainer = shap.TreeExplainer(selected_model)\n",
        "    except Exception:\n",
        "        # Untuk model linear (Logistic Regression)\n",
        "        # SHAP membutuhkan data latih untuk model linear\n",
        "        X_train_processed_sample = shap.sample(X_train_processed, 200, random_state=42)\n",
        "        explainer = shap.KernelExplainer(selected_model.predict_proba, X_train_processed_sample)\n",
        "        print(\"   -> Menggunakan KernelExplainer untuk model non-tree.\")\n",
        "\n",
        "    # --- 4. Hitung Nilai SHAP ---\n",
        "    print(\"Menghitung nilai SHAP pada sampel data uji...\")\n",
        "    X_test_sample = shap.sample(X_test_processed, 1000, random_state=42) # Sampel lebih kecil untuk kecepatan\n",
        "\n",
        "    # Penanganan berbeda untuk TreeExplainer vs KernelExplainer\n",
        "    if isinstance(explainer, shap.explainers.Tree):\n",
        "        shap_values = explainer.shap_values(X_test_sample)\n",
        "    else: # KernelExplainer\n",
        "        shap_values = explainer.shap_values(X_test_sample, l1_reg=\"aic\")[1] # Ambil untuk kelas positif (1)\n",
        "\n",
        "    # --- 5. Dapatkan Nama Fitur ---\n",
        "    print(\"Mendapatkan nama fitur yang telah diproses...\")\n",
        "    num_features_imputed = preprocessor.transformers_[0][1].get_feature_names_out()\n",
        "    cat_features_encoded = preprocessor.transformers_[1][1].get_feature_names_out()\n",
        "    processed_feature_names = np.concatenate([num_features_imputed, cat_features_encoded])\n",
        "\n",
        "    # --- 6. Buat SHAP Summary Plot ---\n",
        "    print(f\"\\n--- SHAP Summary Plot untuk Model {MODEL_TO_ANALYZE} ---\")\n",
        "    X_test_sample_df = pd.DataFrame(X_test_sample, columns=processed_feature_names)\n",
        "    shap.summary_plot(shap_values, X_test_sample_df, plot_type=\"dot\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\n‚ùå ERROR: File model atau preprocessor tidak ditemukan. Pastikan SEL 4.4 sudah dijalankan.\")\n",
        "    print(f\"   Detail Error: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå GAGAL pada SEL 5.1. Error: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "VeX7S24Keseh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 5.2 (REVISI FINAL & ROBUST): Interpretasi Lokal untuk Model Pilihan"
      ],
      "metadata": {
        "id": "Pg0dqfTPREba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 5.2 (REVISI FINAL & ROBUST): Interpretasi Lokal untuk Model Pilihan\n",
        "# =============================================================================\n",
        "print(f\"\\n\\n--- SEL 5.2 (REVISI): Memulai Interpretasi Lokal untuk Model {MODEL_TO_ANALYZE} ---\")\n",
        "\n",
        "# Pastikan semua variabel dari SEL 5.1 sudah ada\n",
        "if 'explainer' in locals() and 'selected_model' in locals():\n",
        "    try:\n",
        "        shap.initjs()\n",
        "\n",
        "        safe_model_name_lower = MODEL_TO_ANALYZE.replace(\" \", \"_\").lower()\n",
        "        VIZ_DIR_SHAP = os.path.join(RESULTS_DIR, f\"visualizations/shap_{safe_model_name_lower}\")\n",
        "        os.makedirs(VIZ_DIR_SHAP, exist_ok=True)\n",
        "        print(f\"üñºÔ∏è  Plot SHAP lokal untuk {MODEL_TO_ANALYZE} akan disimpan di: {VIZ_DIR_SHAP}\")\n",
        "\n",
        "        y_pred_test_full = selected_model.predict(X_test_processed)\n",
        "        tp_indices = np.where((y_test == 1) & (y_pred_test_full == 1))[0]\n",
        "        tn_indices = np.where((y_test == 0) & (y_pred_test_full == 0))[0]\n",
        "\n",
        "        if len(tp_indices) > 0 and len(tn_indices) > 0:\n",
        "            idx_bad_loan = tp_indices[0]\n",
        "            idx_good_loan = tn_indices[0]\n",
        "\n",
        "            # ====================================================================\n",
        "            # --- FUNGSI HELPER YANG SUDAH DIPERBAIKI ---\n",
        "            # ====================================================================\n",
        "            def create_and_save_force_plot(shap_values_single, features_single, idx, plot_type):\n",
        "                \"\"\"Fungsi untuk membuat, menyimpan, dan menampilkan force plot yang bersih dan adaptif.\"\"\"\n",
        "                print(f\"\\n--- Menjelaskan Plot untuk Nasabah {plot_type.title()} (Indeks Uji: {idx}) ---\")\n",
        "\n",
        "                # --- BLOK KODE PERBAIKAN ---\n",
        "                # Cek apakah output SHAP adalah list (punya 2 elemen) atau hanya satu array\n",
        "                expected_value = explainer.expected_value\n",
        "\n",
        "                # Jika expected_value adalah list/array, ambil nilai untuk kelas positif (1)\n",
        "                if isinstance(expected_value, (list, np.ndarray)) and len(np.array(expected_value).shape) > 0:\n",
        "                    expected_value = expected_value[1]\n",
        "\n",
        "                # Jika shap_values_single adalah list/array dari list, ambil nilai untuk kelas positif (1)\n",
        "                # Ini adalah cara aman untuk menangani output [shap_class_0, shap_class_1]\n",
        "                if isinstance(shap_values_single, list) and len(shap_values_single) == 2:\n",
        "                    shap_values_for_plot = shap_values_single[1]\n",
        "                else:\n",
        "                    # Jika bukan list, berarti sudah merupakan array yang benar\n",
        "                    shap_values_for_plot = shap_values_single\n",
        "                # --- AKHIR BLOK PERBAIKAN ---\n",
        "\n",
        "                # Buat plot versi gambar (matplotlib)\n",
        "                plt.figure()\n",
        "                shap.force_plot(expected_value, shap_values_for_plot, features_single,\n",
        "                                feature_names=processed_feature_names, matplotlib=True, show=False, contribution_threshold=0.05)\n",
        "                file_path = os.path.join(VIZ_DIR_SHAP, f\"shap_force_plot_{plot_type}_loan.png\")\n",
        "                plt.savefig(file_path, dpi=150, bbox_inches='tight')\n",
        "                plt.close()\n",
        "                print(f\"   -> Versi gambar disimpan ke: {file_path}\")\n",
        "\n",
        "                # Tampilkan plot versi interaktif (javascript)\n",
        "                print(\"   -> Menampilkan versi interaktif:\")\n",
        "                display(shap.force_plot(expected_value, shap_values_for_plot, features=features_single, feature_names=processed_feature_names))\n",
        "\n",
        "            # --- Proses Penjelasan (tidak ada perubahan di sini) ---\n",
        "            # Proses untuk nasabah berisiko\n",
        "            row_bad_loan_numpy = X_test_processed[idx_bad_loan:idx_bad_loan+1, :]\n",
        "            shap_values_bad = explainer.shap_values(row_bad_loan_numpy)\n",
        "            create_and_save_force_plot(shap_values_bad, row_bad_loan_numpy, idx_bad_loan, \"bad\")\n",
        "\n",
        "            # Proses untuk nasabah aman\n",
        "            row_good_loan_numpy = X_test_processed[idx_good_loan:idx_good_loan+1, :]\n",
        "            shap_values_good = explainer.shap_values(row_good_loan_numpy)\n",
        "            create_and_save_force_plot(shap_values_good, row_good_loan_numpy, idx_good_loan, \"good\")\n",
        "\n",
        "        else:\n",
        "            print(f\"Tidak dapat menemukan contoh True Positive atau True Negative untuk dijelaskan dengan model {MODEL_TO_ANALYZE}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå GAGAL pada SEL 5.2. Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: Variabel dari SEL 5.1 tidak ditemukan. Jalankan SEL 5.1 terlebih dahulu.\")"
      ],
      "metadata": {
        "id": "goqYh_1zRC02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 5.3 (MANDIRI): Pemuatan Artefak & Analisis Dampak Finansial"
      ],
      "metadata": {
        "id": "TCuNw5EGRGnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 5.3 (MANDIRI): Pemuatan Artefak & Analisis Dampak Finansial\n",
        "# =============================================================================\n",
        "print(\"--- SEL 5.3 (MANDIRI): Memuat Artefak dan Memulai Analisis Finansial ---\")\n",
        "\n",
        "try:\n",
        "    # --- BAGIAN 0: PEMUATAN SEMUA KOMPONEN YANG DIPERLUKAN ---\n",
        "    print(\"üíæ Memuat semua artefak yang diperlukan dari disk...\")\n",
        "\n",
        "    # 1. Tentukan path ke model, preprocessor, dan data\n",
        "    model_path = \"/content/drive/MyDrive/Colab_Projects/Credit_Risk_Prediction#1/02_models/champion_model_CatBoost.joblib\"\n",
        "    preprocessor_path = os.path.join(MODELS_DIR, 'preprocessor.joblib')\n",
        "    data_path = os.path.join(DATA_PROCESSED_DIR, \"loan_data_model_ready.csv\")\n",
        "\n",
        "    # 2. Muat model dan preprocessor\n",
        "    champion_model = joblib.load(model_path)\n",
        "    preprocessor = joblib.load(preprocessor_path)\n",
        "    print(f\"   -> Model '{champion_model.__class__.__name__}' berhasil dimuat.\")\n",
        "    print(\"   -> Preprocessor berhasil dimuat.\")\n",
        "\n",
        "    # 3. Muat ulang data bersih dan lakukan train-test split yang sama persis\n",
        "    # Ini penting untuk memastikan X_test dan y_test konsisten dengan yang digunakan saat training\n",
        "    df_model_ready = pd.read_csv(data_path)\n",
        "\n",
        "    # Hapus kolom 'Unnamed: 0' jika masih ada di file CSV sebagai pengaman\n",
        "    if 'Unnamed: 0' in df_model_ready.columns:\n",
        "        df_model_ready.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "\n",
        "    X = df_model_ready.drop(columns=['credit_risk'])\n",
        "    y = df_model_ready['credit_risk']\n",
        "\n",
        "    # Gunakan random_state=42 agar splitnya selalu sama\n",
        "    _, X_test, _, y_test = train_test_split(\n",
        "        X, y, test_size=0.1, random_state=42, stratify=y\n",
        "    )\n",
        "    print(\"   -> Data uji (X_test, y_test) berhasil dibuat ulang secara konsisten.\")\n",
        "\n",
        "    # 4. Proses X_test menggunakan preprocessor yang sudah dimuat\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "    print(\"   -> X_test berhasil diproses menggunakan preprocessor.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "\n",
        "    # --- BAGIAN 1: Siapkan DataFrame untuk Analisis Finansial ---\n",
        "    df_finance = X_test.copy()\n",
        "    df_finance['credit_risk_actual'] = y_test\n",
        "    df_finance['credit_risk_predicted'] = champion_model.predict(X_test_processed)\n",
        "\n",
        "    # --- BAGIAN 2: Hitung Potensi Keuntungan dan Kerugian untuk SETIAP Pinjaman ---\n",
        "    df_finance['potential_profit'] = (df_finance['int_rate'] / 100) * df_finance['loan_amnt']\n",
        "    df_finance['potential_loss'] = df_finance['loan_amnt']\n",
        "\n",
        "    print(\"‚úÖ DataFrame finansial telah dibuat dengan potensi profit/loss per baris.\")\n",
        "    display(df_finance[['loan_amnt', 'int_rate', 'potential_profit', 'potential_loss', 'credit_risk_actual', 'credit_risk_predicted']].head())\n",
        "\n",
        "    # --- BAGIAN 3: Hitung Dampak Finansial dari Strategi Model (Tingkat Individu) ---\n",
        "    print(\"\\n--- Menghitung Dampak Finansial dengan Model (Tingkat Individu) ---\")\n",
        "    profit_from_tn = df_finance[(df_finance['credit_risk_predicted'] == 0) & (df_finance['credit_risk_actual'] == 0)]['potential_profit'].sum()\n",
        "    loss_from_fn = df_finance[(df_finance['credit_risk_predicted'] == 0) & (df_finance['credit_risk_actual'] == 1)]['potential_loss'].sum()\n",
        "    net_impact_with_model = profit_from_tn - loss_from_fn\n",
        "\n",
        "    print(f\"   Total Keuntungan dari Pinjaman yang Disetujui & Lunas (TN): ${profit_from_tn:,.2f}\")\n",
        "    print(f\"   Total Kerugian dari Pinjaman yang Disetujui & Gagal Bayar (FN): ${loss_from_fn:,.2f}\")\n",
        "    print(f\"   => Dampak Finansial Bersih (Dengan Model): ${net_impact_with_model:,.2f}\")\n",
        "\n",
        "    opportunity_cost_fp = df_finance[(df_finance['credit_risk_predicted'] == 1) & (df_finance['credit_risk_actual'] == 0)]['potential_profit'].sum()\n",
        "    print(f\"   (Opportunity Cost dari Penolakan yang Salah (FP): ${opportunity_cost_fp:,.2f})\")\n",
        "    avoided_loss_tp = df_finance[(df_finance['credit_risk_predicted'] == 1) & (df_finance['credit_risk_actual'] == 1)]['potential_loss'].sum()\n",
        "    print(f\"   (Kerugian yang Berhasil Dihindari (TP): ${avoided_loss_tp:,.2f})\")\n",
        "\n",
        "    # --- BAGIAN 4: Hitung Dampak Finansial Skenario Tanpa Model (Setujui Semua) ---\n",
        "    print(\"\\n--- Menghitung Dampak Finansial Skenario Tanpa Model (Setujui Semua) ---\")\n",
        "    profit_no_model = df_finance[df_finance['credit_risk_actual'] == 0]['potential_profit'].sum()\n",
        "    loss_no_model = df_finance[df_finance['credit_risk_actual'] == 1]['potential_loss'].sum()\n",
        "    net_impact_no_model = profit_no_model - loss_no_model\n",
        "\n",
        "    print(f\"   Total Keuntungan jika semua Good Loan disetujui: ${profit_no_model:,.2f}\")\n",
        "    print(f\"   Total Kerugian jika semua Bad Loan disetujui: ${loss_no_model:,.2f}\")\n",
        "    print(f\"   => Dampak Finansial Bersih (Tanpa Model): ${net_impact_no_model:,.2f}\")\n",
        "\n",
        "    # --- BAGIAN 5: Ringkasan dan Nilai Tambah Model ---\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"      RINGKASAN DAMPAK FINANSIAL (ANALISIS TINGKAT INDIVIDU)\")\n",
        "    print(\"=\"*60)\n",
        "    value_add_of_model = net_impact_with_model - net_impact_no_model\n",
        "\n",
        "    print(f\"Dampak Bersih Tanpa Model: ${net_impact_no_model:,.2f}\")\n",
        "    print(f\"Dampak Bersih Dengan Model: ${net_impact_with_model:,.2f}\")\n",
        "    print(\"-\" * 60)\n",
        "    if value_add_of_model > 0:\n",
        "        print(f\"‚úÖ Nilai Tambah Finansial dari Model: ${value_add_of_model:,.2f}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Model ini secara finansial merugikan sebesar: ${-value_add_of_model:,.2f}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\n‚ùå GAGAL: File tidak ditemukan. Pastikan path sudah benar dan file ada di Google Drive.\")\n",
        "    print(f\"   Detail Error: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå GAGAL pada SEL 5.3 (Mandiri). Error: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "oOAy_SHA7aWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 5.4 (DINAMIS): Perbandingan Finansial Otomatis dari Model Tersimpan"
      ],
      "metadata": {
        "id": "GJyWi5cARJfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 5.4 (DINAMIS): Perbandingan Finansial Otomatis dari Model Tersimpan\n",
        "# =============================================================================\n",
        "print(\"--- SEL 5.4 (DINAMIS): Memulai Perbandingan Finansial Otomatis ---\")\n",
        "\n",
        "try:\n",
        "    # --- 1. Definisikan Ulang Asumsi Finansial (Konsisten) ---\n",
        "    print(\"üí∞ Memuat ulang asumsi finansial rata-rata...\")\n",
        "    df_raw_for_finance = pd.read_csv(os.path.join(DATA_RAW_DIR, \"loan_data_2007_2014.csv\"))\n",
        "\n",
        "    avg_profit_per_good_loan = df_raw_for_finance[df_raw_for_finance['loan_status'] == 'Fully Paid']['total_rec_int'].mean()\n",
        "    avg_loss_per_bad_loan = df_raw_for_finance[df_raw_for_finance['loan_status'] == 'Charged Off']['loan_amnt'].mean()\n",
        "\n",
        "    print(f\"   -> Rata-rata Keuntungan per Pinjaman Lunas: ${avg_profit_per_good_loan:,.2f}\")\n",
        "    print(f\"   -> Rata-rata Kerugian per Pinjaman Gagal Bayar: ${avg_loss_per_bad_loan:,.2f}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # --- 2. Muat Ulang Data Uji yang Konsisten ---\n",
        "    # (Pastikan X_test_processed dan y_test sudah ada dari sel sebelumnya, atau muat ulang jika perlu)\n",
        "    if 'X_test_processed' not in globals() or 'y_test' not in globals():\n",
        "        print(\"‚ö†Ô∏è  Data uji tidak ditemukan di memori, memuat ulang...\")\n",
        "        # Kode ini adalah pengaman jika Anda menjalankan sel ini di sesi baru\n",
        "        data_path = os.path.join(DATA_PROCESSED_DIR, \"loan_data_model_ready.csv\")\n",
        "        df_model_ready = pd.read_csv(data_path)\n",
        "        if 'Unnamed: 0' in df_model_ready.columns: df_model_ready.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "        X = df_model_ready.drop(columns=['credit_risk'])\n",
        "        y = df_model_ready['credit_risk']\n",
        "        _, _, _, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
        "\n",
        "        X_test_processed = np.load(os.path.join(DATA_PROCESSED_DIR, 'X_test_processed.npy'))\n",
        "        print(\"   -> Data uji berhasil dimuat dari file .npy.\")\n",
        "\n",
        "    # --- 3. Muat SEMUA Model yang Telah Di-tuning & Hitung CM secara Dinamis ---\n",
        "    print(\"ü§ñ Memuat semua model yang telah di-tuning dan menghitung Confusion Matrix...\")\n",
        "\n",
        "    model_names = ['Logistic Regression', 'LightGBM', 'XGBoost', 'CatBoost']\n",
        "    financial_results = []\n",
        "\n",
        "    for model_name in model_names:\n",
        "        safe_model_name = model_name.replace(\" \", \"_\")\n",
        "        # Sesuaikan nama file jika Anda menggunakan 'champion_model' untuk CatBoost\n",
        "        if model_name == 'CatBoost' and not os.path.exists(os.path.join(MODELS_DIR, f'tuned_model_{safe_model_name}.joblib')):\n",
        "             model_path = os.path.join(MODELS_DIR, f'champion_model_{safe_model_name}.joblib')\n",
        "        else:\n",
        "             model_path = os.path.join(MODELS_DIR, f'tuned_model_{safe_model_name}.joblib')\n",
        "\n",
        "        try:\n",
        "            # Muat model\n",
        "            model = joblib.load(model_path)\n",
        "            print(f\"\\n   -> Model '{model_name}' dimuat.\")\n",
        "\n",
        "            # Buat prediksi\n",
        "            y_pred = model.predict(X_test_processed)\n",
        "\n",
        "            # Hitung confusion matrix secara otomatis\n",
        "            tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "            print(f\"      CM: TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
        "\n",
        "            # --- 4. Hitung Dampak Finansial untuk Model Ini ---\n",
        "            total_profit = tn * avg_profit_per_good_loan\n",
        "            total_loss = fn * avg_loss_per_bad_loan\n",
        "            net_impact = total_profit - total_loss\n",
        "            opportunity_cost = fp * avg_profit_per_good_loan\n",
        "            avoided_loss = tp * avg_loss_per_bad_loan\n",
        "\n",
        "            financial_results.append({\n",
        "                'Model': model_name,\n",
        "                'Dampak Bersih': net_impact,\n",
        "                'Total Keuntungan (TN)': total_profit,\n",
        "                'Total Kerugian (FN)': total_loss,\n",
        "                'Opportunity Cost (FP)': opportunity_cost,\n",
        "                'Kerugian Dihindari (TP)': avoided_loss\n",
        "            })\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"   -> ‚ùå PERINGATAN: File model untuk '{model_name}' tidak ditemukan di {model_path}. Model ini akan dilewati.\")\n",
        "            continue\n",
        "\n",
        "    # --- 5. Buat Tabel Perbandingan ---\n",
        "    if not financial_results:\n",
        "        print(\"\\n‚ùå ERROR: Tidak ada model yang berhasil dimuat. Analisis tidak dapat dilanjutkan.\")\n",
        "    else:\n",
        "        df_comparison = pd.DataFrame(financial_results)\n",
        "        df_comparison_sorted = df_comparison.sort_values(by='Dampak Bersih', ascending=False)\n",
        "\n",
        "        # Format angka agar mudah dibaca\n",
        "        for col in ['Dampak Bersih', 'Total Keuntungan (TN)', 'Total Kerugian (FN)', 'Opportunity Cost (FP)', 'Kerugian Dihindari (TP)']:\n",
        "            df_comparison_sorted[col] = df_comparison_sorted[col].apply(lambda x: f\"${x:,.2f}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"      TABEL PERBANDINGAN DAMPAK FINANSIAL (DINAMIS)\")\n",
        "        print(\"=\"*80)\n",
        "        display(df_comparison_sorted)\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # --- 6. Analisis dan Rekomendasi Otomatis ---\n",
        "        best_financial_model = df_comparison.sort_values(by='Dampak Bersih', ascending=False).iloc[0]\n",
        "\n",
        "        print(\"\\n--- Analisis dan Rekomendasi ---\")\n",
        "        print(f\"Model terbaik dari perspektif finansial (keuntungan bersih terbesar) adalah '{best_financial_model['Model']}'.\")\n",
        "        print(f\"\\nModel '{best_financial_model['Model']}' menghasilkan keuntungan bersih sebesar ${best_financial_model['Dampak Bersih']:,.2f}.\")\n",
        "\n",
        "        print(\"\\nREKOMENDASI: Berdasarkan analisis finansial, model '{0}' direkomendasikan untuk implementasi karena memberikan dampak paling positif terhadap keuangan perusahaan.\".format(best_financial_model['Model']))\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\n‚ùå ERROR: File data mentah atau data uji tidak ditemukan. Pastikan semua path benar.\")\n",
        "    print(f\"   Detail: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå GAGAL pada SEL 5.4 (Dinamis). Error: {e}\")\n"
      ],
      "metadata": {
        "id": "JzaMD09YCQs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagian 6: Analisis Robustness"
      ],
      "metadata": {
        "id": "_Lcgy1y-RONl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 6.0 (BARU): Analisis Robustness Model Juara"
      ],
      "metadata": {
        "id": "_sGh4nsWRNc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 6.0 (BARU): Analisis Robustness Model Juara\n",
        "# =============================================================================\n",
        "print(\"--- SEL 6.0: Memulai Analisis Robustness untuk Model Juara ---\")\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    # --- 1. Konfigurasi dan Pemuatan Aset ---\n",
        "    MODEL_TO_VALIDATE = 'CatBoost' # Fokus pada model juara\n",
        "    N_SPLITS = 10 # Gunakan 10-fold CV untuk analisis yang lebih detail\n",
        "\n",
        "    print(f\"üéØ Menganalisis robustness untuk model: '{MODEL_TO_VALIDATE}'\")\n",
        "\n",
        "    # Muat model yang sudah di-tuning\n",
        "    safe_model_name = MODEL_TO_VALIDATE.replace(\" \", \"_\")\n",
        "    model_path = os.path.join(MODELS_DIR, f'tuned_model_{safe_model_name}.joblib')\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "    # Muat data yang sudah diproses (sebelum di-split)\n",
        "    data_path = os.path.join(DATA_PROCESSED_DIR, \"loan_data_model_ready.csv\")\n",
        "    df_model_ready = pd.read_csv(data_path)\n",
        "    if 'Unnamed: 0' in df_model_ready.columns: df_model_ready.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "\n",
        "    X = df_model_ready.drop(columns=['credit_risk'])\n",
        "    y = df_model_ready['credit_risk']\n",
        "\n",
        "    # Muat preprocessor\n",
        "    preprocessor_path = os.path.join(MODELS_DIR, 'preprocessor.joblib')\n",
        "    preprocessor = joblib.load(preprocessor_path)\n",
        "\n",
        "    # Proses seluruh data X\n",
        "    X_processed = preprocessor.transform(X)\n",
        "    print(\"   -> Semua aset (model, data, preprocessor) berhasil dimuat dan diproses.\")\n",
        "\n",
        "    # --- 2. Metode 1: Stabilitas Kinerja pada Cross-Validation ---\n",
        "    print(\"\\n--- Metode 1: Mengukur Stabilitas Kinerja pada Cross-Validation ---\")\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
        "    fold_metrics = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_processed, y)):\n",
        "        # Ambil data untuk fold ini\n",
        "        X_val_fold, y_val_fold = X_processed[val_idx], y.iloc[val_idx]\n",
        "\n",
        "        # Buat prediksi (model sudah dilatih, kita hanya memvalidasi)\n",
        "        y_pred_proba = model.predict_proba(X_val_fold)[:, 1]\n",
        "\n",
        "        # Hitung metrik\n",
        "        auc = roc_auc_score(y_val_fold, y_pred_proba)\n",
        "        gini = 2 * auc - 1\n",
        "        ks = calculate_ks(y_val_fold, y_pred_proba)\n",
        "\n",
        "        fold_metrics.append({'Fold': fold + 1, 'Gini': gini, 'KS': ks})\n",
        "\n",
        "    df_fold_metrics = pd.DataFrame(fold_metrics)\n",
        "\n",
        "    print(f\"\\nKinerja Gini di {N_SPLITS} Folds:\")\n",
        "    display(df_fold_metrics[['Fold', 'Gini']])\n",
        "\n",
        "    # Hitung statistik stabilitas\n",
        "    gini_std = df_fold_metrics['Gini'].std()\n",
        "    print(f\"\\nStatistik Stabilitas:\")\n",
        "    print(f\"   -> Rata-rata Gini: {df_fold_metrics['Gini'].mean():.4f}\")\n",
        "    print(f\"   -> Standar Deviasi Gini: {gini_std:.4f}\")\n",
        "\n",
        "    if gini_std < 0.03:\n",
        "        print(\"   -> ‚úÖ Stabilitas SANGAT BAIK (Std Dev < 0.03). Model konsisten di berbagai sampel data.\")\n",
        "    elif gini_std < 0.05:\n",
        "        print(\"   -> ‚úÖ Stabilitas BAIK (Std Dev < 0.05). Model cukup konsisten.\")\n",
        "    else:\n",
        "        print(\"   -> ‚ö†Ô∏è  PERINGATAN: Stabilitas KURANG (Std Dev > 0.05). Kinerja model mungkin tidak stabil.\")\n",
        "\n",
        "\n",
        "    # --- 3. Metode 2: Analisis Kinerja pada Segmen Data ---\n",
        "    print(\"\\n\\n--- Metode 2: Mengukur Kinerja pada Segmen Data Penting ---\")\n",
        "\n",
        "    # Pilih beberapa fitur kategorikal penting untuk segmentasi\n",
        "    # Kita gunakan data X sebelum diproses untuk mendapatkan nilai asli\n",
        "    segments_to_analyze = ['grade', 'home_ownership', 'purpose']\n",
        "\n",
        "    # Buat prediksi probabilitas untuk seluruh dataset\n",
        "    y_full_pred_proba = model.predict_proba(X_processed)[:, 1]\n",
        "\n",
        "    segment_analysis_results = []\n",
        "\n",
        "    for segment_col in segments_to_analyze:\n",
        "        if segment_col in X.columns:\n",
        "            print(f\"\\n--- Menganalisis segmen berdasarkan '{segment_col}' ---\")\n",
        "            unique_values = X[segment_col].unique()\n",
        "\n",
        "            for value in unique_values:\n",
        "                # Dapatkan indeks untuk segmen spesifik ini\n",
        "                segment_indices = X[X[segment_col] == value].index\n",
        "\n",
        "                # Filter y dan prediksi untuk segmen ini\n",
        "                y_segment = y[segment_indices]\n",
        "                y_pred_proba_segment = y_full_pred_proba[segment_indices]\n",
        "\n",
        "                # Pastikan ada cukup data dan kedua kelas ada di segmen\n",
        "                if len(y_segment) > 100 and len(np.unique(y_segment)) == 2:\n",
        "                    auc_segment = roc_auc_score(y_segment, y_pred_proba_segment)\n",
        "                    gini_segment = 2 * auc_segment - 1\n",
        "\n",
        "                    segment_analysis_results.append({\n",
        "                        'Segment_Feature': segment_col,\n",
        "                        'Segment_Value': value,\n",
        "                        'Gini': gini_segment,\n",
        "                        'Data_Count': len(y_segment)\n",
        "                    })\n",
        "\n",
        "    df_segment_analysis = pd.DataFrame(segment_analysis_results)\n",
        "\n",
        "    # Tampilkan hasil analisis segmen\n",
        "    for segment_col in segments_to_analyze:\n",
        "        if segment_col in df_segment_analysis['Segment_Feature'].values:\n",
        "            print(f\"\\nKinerja Gini per Segmen '{segment_col}':\")\n",
        "            display(df_segment_analysis[df_segment_analysis['Segment_Feature'] == segment_col].sort_values(by='Gini', ascending=False))\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\n‚ùå ERROR: File model atau data tidak ditemukan. Pastikan SEL 4.4 sudah dijalankan.\")\n",
        "    print(f\"   Detail: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå GAGAL pada SEL 4.10. Error: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8_AXFMRS-S3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 6.1 (BARU): Analisis Robustness terhadap Noise (Gangguan Data)"
      ],
      "metadata": {
        "id": "-ZIDdhlRRdP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 6.1 (BARU): Analisis Robustness terhadap Noise (Gangguan Data)\n",
        "# =============================================================================\n",
        "print(\"--- SEL 6.1: Memulai Analisis Robustness terhadap Noise ---\")\n",
        "\n",
        "try:\n",
        "    # --- 1. Konfigurasi dan Pemuatan Aset ---\n",
        "    MODEL_TO_TEST = 'CatBoost'\n",
        "    NOISE_LEVELS = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10] # Level noise: 1%, 3%, 5%, 10% dari std dev\n",
        "\n",
        "    print(f\"üéØ Menganalisis ketahanan noise untuk model: '{MODEL_TO_TEST}'\")\n",
        "\n",
        "    # Muat model, preprocessor, dan data uji\n",
        "    safe_model_name = MODEL_TO_TEST.replace(\" \", \"_\")\n",
        "    model_path = os.path.join(MODELS_DIR, f'tuned_model_{safe_model_name}.joblib')\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "    preprocessor_path = os.path.join(MODELS_DIR, 'preprocessor.joblib')\n",
        "    preprocessor = joblib.load(preprocessor_path)\n",
        "\n",
        "    if 'X_test' not in globals() or 'y_test' not in globals():\n",
        "        print(\"‚ö†Ô∏è  Data uji (X_test) tidak ditemukan, membuat ulang...\")\n",
        "        data_path = os.path.join(DATA_PROCESSED_DIR, \"loan_data_model_ready.csv\")\n",
        "        df_model_ready = pd.read_csv(data_path)\n",
        "        if 'Unnamed: 0' in df_model_ready.columns: df_model_ready.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "        X = df_model_ready.drop(columns=['credit_risk'])\n",
        "        y = df_model_ready['credit_risk']\n",
        "        _, X_test, _, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
        "        print(\"   -> Data uji berhasil dibuat ulang.\")\n",
        "\n",
        "    print(\"   -> Aset (model, data, preprocessor) berhasil dimuat.\")\n",
        "\n",
        "    # --- 2. Hitung Kinerja Baseline (Tanpa Noise) ---\n",
        "    print(\"\\n--- Menghitung Kinerja Baseline (Tanpa Noise) ---\")\n",
        "    X_test_processed_original = preprocessor.transform(X_test)\n",
        "    y_pred_proba_original = model.predict_proba(X_test_processed_original)[:, 1]\n",
        "\n",
        "    baseline_auc = roc_auc_score(y_test, y_pred_proba_original)\n",
        "    baseline_gini = 2 * baseline_auc - 1\n",
        "\n",
        "    print(f\"   -> Gini Baseline: {baseline_gini:.4f}\")\n",
        "\n",
        "    noise_results = [{'Noise_Level': 0, 'Gini': baseline_gini, 'Gini_Drop': 0.0}]\n",
        "\n",
        "    # --- 3. Tambahkan Noise dan Evaluasi Ulang ---\n",
        "    print(\"\\n--- Menambahkan Noise pada Fitur Numerik dan Mengevaluasi Ulang ---\")\n",
        "\n",
        "    numeric_features = X_test.select_dtypes(include=np.number).columns\n",
        "\n",
        "    for noise_level in NOISE_LEVELS:\n",
        "        print(f\"   -> Menguji dengan tingkat noise: {noise_level*100:.0f}%...\")\n",
        "        X_test_noisy = X_test.copy()\n",
        "\n",
        "        # Tambahkan noise ke setiap fitur numerik\n",
        "        for col in numeric_features:\n",
        "            # Hitung standar deviasi dari kolom\n",
        "            std_dev = X_test_noisy[col].std()\n",
        "            # Hasilkan noise acak berdasarkan std dev dan level noise\n",
        "            noise = np.random.normal(0, std_dev * noise_level, X_test_noisy.shape[0])\n",
        "            # Tambahkan noise ke data\n",
        "            X_test_noisy[col] = X_test_noisy[col] + noise\n",
        "\n",
        "        # Proses data yang sudah terganggu dengan preprocessor yang sama\n",
        "        X_test_noisy_processed = preprocessor.transform(X_test_noisy)\n",
        "\n",
        "        # Buat prediksi dengan model\n",
        "        y_pred_proba_noisy = model.predict_proba(X_test_noisy_processed)[:, 1]\n",
        "\n",
        "        # Hitung metrik\n",
        "        noisy_auc = roc_auc_score(y_test, y_pred_proba_noisy)\n",
        "        noisy_gini = 2 * noisy_auc - 1\n",
        "\n",
        "        # Hitung penurunan kinerja\n",
        "        gini_drop = baseline_gini - noisy_gini\n",
        "\n",
        "        noise_results.append({\n",
        "            'Noise_Level': noise_level,\n",
        "            'Gini': noisy_gini,\n",
        "            'Gini_Drop': gini_drop\n",
        "        })\n",
        "\n",
        "    # --- 4. Tampilkan Hasil Analisis Noise ---\n",
        "    df_noise_analysis = pd.DataFrame(noise_results)\n",
        "    df_noise_analysis['Gini_Drop_Percent'] = (df_noise_analysis['Gini_Drop'] / baseline_gini) * 100\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"      HASIL ANALISIS KETAHANAN TERHADAP NOISE\")\n",
        "    print(\"=\"*80)\n",
        "    display(df_noise_analysis)\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # --- 5. Plot Hasil untuk Visualisasi ---\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(df_noise_analysis['Noise_Level']*100, df_noise_analysis['Gini'], marker='o', linestyle='-')\n",
        "    plt.title(f'Kinerja Gini vs. Tingkat Noise untuk Model {MODEL_TO_TEST}')\n",
        "    plt.xlabel('Tingkat Noise Ditambahkan (%)')\n",
        "    plt.ylabel('Skor Gini')\n",
        "    plt.grid(True)\n",
        "    plt.ylim(bottom=df_noise_analysis['Gini'].min() - 0.02, top=baseline_gini + 0.01) # Atur batas y\n",
        "    plt.show()\n",
        "\n",
        "    # --- 6. Berikan Kesimpulan Otomatis ---\n",
        "    final_drop_percent = df_noise_analysis.iloc[-1]['Gini_Drop_Percent']\n",
        "    print(\"\\n--- Kesimpulan Robustness terhadap Noise ---\")\n",
        "    if final_drop_percent < 5:\n",
        "        print(f\"   -> ‚úÖ Model SANGAT ROBUST. Penurunan Gini hanya {final_drop_percent:.2f}% bahkan pada tingkat noise 10%.\")\n",
        "    elif final_drop_percent < 10:\n",
        "        print(f\"   -> ‚úÖ Model CUKUP ROBUST. Penurunan Gini sebesar {final_drop_percent:.2f}% pada tingkat noise 10%. Kinerja masih dapat diterima.\")\n",
        "    else:\n",
        "        print(f\"   -> ‚ö†Ô∏è  PERINGATAN: Model SENSITIF terhadap noise. Penurunan Gini mencapai {final_drop_percent:.2f}% pada tingkat noise 10%. Perlu hati-hati dalam implementasi.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\n‚ùå ERROR: File model atau data tidak ditemukan. Pastikan SEL 4.4 sudah dijalankan.\")\n",
        "    print(f\"   Detail: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå GAGAL pada SEL 4.11. Error: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4e2qPXglA_lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEL 6.2 (BARU): Analisis Robustness terhadap Pergeseran Data (PSI)"
      ],
      "metadata": {
        "id": "E7eA6dNgRlt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SEL 6.2 (BARU): Analisis Robustness terhadap Pergeseran Data (PSI)\n",
        "# =============================================================================\n",
        "print(\"--- SEL 4.12: Memulai Analisis Stabilitas Populasi (PSI) ---\")\n",
        "\n",
        "try:\n",
        "    # --- 1. Konfigurasi dan Pemuatan Aset ---\n",
        "    MODEL_TO_VALIDATE = 'CatBoost'\n",
        "    N_BINS = 10 # Jumlah keranjang untuk kalkulasi PSI\n",
        "\n",
        "    print(f\"üéØ Menganalisis stabilitas populasi untuk model: '{MODEL_TO_VALIDATE}'\")\n",
        "\n",
        "    # Muat model, preprocessor, dan data\n",
        "    safe_model_name = MODEL_TO_VALIDATE.replace(\" \", \"_\")\n",
        "    model_path = os.path.join(MODELS_DIR, f'tuned_model_{safe_model_name}.joblib')\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "    preprocessor_path = os.path.join(MODELS_DIR, 'preprocessor.joblib')\n",
        "    preprocessor = joblib.load(preprocessor_path)\n",
        "\n",
        "    # Kita butuh X_train dan X_test yang asli (sebelum diproses)\n",
        "    if 'X_train' not in globals() or 'X_test' not in globals():\n",
        "        print(\"‚ö†Ô∏è  Data latih/uji tidak ditemukan, membuat ulang...\")\n",
        "        data_path = os.path.join(DATA_PROCESSED_DIR, \"loan_data_model_ready.csv\")\n",
        "        df_model_ready = pd.read_csv(data_path)\n",
        "        if 'Unnamed: 0' in df_model_ready.columns: df_model_ready.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "        X = df_model_ready.drop(columns=['credit_risk'])\n",
        "        y = df_model_ready['credit_risk']\n",
        "        X_train, X_test, _, _ = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
        "        print(\"   -> Data latih dan uji berhasil dibuat ulang.\")\n",
        "\n",
        "    print(\"   -> Aset (model, data, preprocessor) berhasil dimuat.\")\n",
        "\n",
        "    # --- 2. Fungsi untuk Menghitung PSI ---\n",
        "    def calculate_psi(base_series, new_series, bins=N_BINS):\n",
        "        \"\"\"Menghitung Population Stability Index (PSI) antara dua series data.\"\"\"\n",
        "\n",
        "        # Tentukan batas bin berdasarkan data referensi (base)\n",
        "        base_series_non_nan = base_series.dropna()\n",
        "        bin_edges = np.histogram_bin_edges(base_series_non_nan, bins=bins)\n",
        "\n",
        "        # Hitung persentase di setiap bin untuk data base\n",
        "        base_counts = np.histogram(base_series_non_nan, bins=bin_edges)[0]\n",
        "        base_dist = base_counts / len(base_series_non_nan)\n",
        "\n",
        "        # Hitung persentase di setiap bin untuk data baru\n",
        "        new_series_non_nan = new_series.dropna()\n",
        "        new_counts = np.histogram(new_series_non_nan, bins=bin_edges)[0]\n",
        "        new_dist = new_counts / len(new_series_non_nan)\n",
        "\n",
        "        # Hindari pembagian dengan nol\n",
        "        base_dist = np.where(base_dist == 0, 0.0001, base_dist)\n",
        "        new_dist = np.where(new_dist == 0, 0.0001, new_dist)\n",
        "\n",
        "        # Hitung PSI\n",
        "        psi_value = np.sum((new_dist - base_dist) * np.log(new_dist / base_dist))\n",
        "\n",
        "        return psi_value\n",
        "\n",
        "    # --- 3. Hitung PSI untuk Skor Model ---\n",
        "    print(\"\\n--- Menghitung PSI untuk Skor Prediksi Model ---\")\n",
        "\n",
        "    # Dapatkan skor probabilitas untuk data latih dan uji\n",
        "    X_train_processed = preprocessor.transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "    train_scores = pd.Series(model.predict_proba(X_train_processed)[:, 1])\n",
        "    test_scores = pd.Series(model.predict_proba(X_test_processed)[:, 1])\n",
        "\n",
        "    model_psi = calculate_psi(train_scores, test_scores)\n",
        "\n",
        "    print(f\"   -> PSI Skor Model: {model_psi:.4f}\")\n",
        "    if model_psi < 0.1:\n",
        "        print(\"   -> ‚úÖ Stabilitas SANGAT BAIK. Distribusi skor model konsisten antara data latih dan uji.\")\n",
        "    elif model_psi < 0.25:\n",
        "        print(\"   -> ‚úÖ Stabilitas CUKUP. Ada sedikit pergeseran pada distribusi skor.\")\n",
        "    else:\n",
        "        print(\"   -> ‚ö†Ô∏è  PERINGATAN: Stabilitas BURUK. Distribusi skor telah bergeser signifikan.\")\n",
        "\n",
        "    # --- 4. Hitung CSI untuk Fitur-fitur Penting ---\n",
        "    print(\"\\n--- Menghitung CSI (Characteristic Stability Index) untuk Fitur Penting ---\")\n",
        "\n",
        "    features_to_check = ['int_rate', 'loan_amnt', 'dti', 'annual_inc', 'credit_history_length_years']\n",
        "    csi_results = []\n",
        "\n",
        "    for feature in features_to_check:\n",
        "        if feature in X_train.columns:\n",
        "            csi_value = calculate_psi(X_train[feature], X_test[feature])\n",
        "            csi_results.append({'Feature': feature, 'CSI': csi_value})\n",
        "\n",
        "    df_csi = pd.DataFrame(csi_results)\n",
        "\n",
        "    print(\"\\nHasil Analisis CSI:\")\n",
        "    display(df_csi.sort_values(by='CSI', ascending=False))\n",
        "\n",
        "    # Berikan kesimpulan umum\n",
        "    if (df_csi['CSI'] > 0.25).any():\n",
        "        print(\"\\n‚ö†Ô∏è  PERINGATAN: Ditemukan fitur dengan pergeseran distribusi yang signifikan (CSI > 0.25).\")\n",
        "        print(\"   Fitur-fitur ini perlu diinvestigasi lebih lanjut karena perilakunya berubah antara data latih dan uji.\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ Stabilitas fitur-fitur utama terlihat baik (semua CSI < 0.25).\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\n‚ùå ERROR: File model atau data tidak ditemukan. Pastikan SEL 4.4 sudah dijalankan.\")\n",
        "    print(f\"   Detail: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå GAGAL pada SEL 4.12. Error: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4fDFz94iDpAP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}